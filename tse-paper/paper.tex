%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended,
  envcountsect,]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.

% algorithmic from the tse template:
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx

\usepackage{etoolbox}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

\makeatletter
% start with some helper code
% This is the vertical rule that is inserted
\newcommand*{\algrule}[1][\algorithmicindent]{%
  \makebox[#1][l]{%
    \hspace*{.2em}% <------------- This is where the rule starts from
    \vrule height .75\baselineskip depth .25\baselineskip
  }
}

\newcount\ALG@printindent@tempcnta
\def\ALG@printindent{%
    \ifnum \theALG@nested>0% is there anything to print
    \ifx\ALG@text\ALG@x@notext% is this an end group without any text?
    % do nothing
    \else
    \unskip
    % draw a rule for each indent level
    \ALG@printindent@tempcnta=1
    \loop
    \algrule[\csname ALG@ind@\the\ALG@printindent@tempcnta\endcsname]%
    \advance \ALG@printindent@tempcnta 1
    \ifnum \ALG@printindent@tempcnta<\numexpr\theALG@nested+1\relax
    \repeat
    \fi
    \fi
}
% the following line injects our new indent handling code in place of the default spacing
\patchcmd{\ALG@doentity}{\noindent\hskip\ALG@tlm}{\ALG@printindent}{}{\errmessage{failed to patch}}
\patchcmd{\ALG@doentity}{\item[]\nointerlineskip}{}{}{} % no spurious vertical space
% end vertical rule patch for algorithmicx
\makeatother

\usepackage[dvipsnames]{xcolor}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}
\renewcommand{\arraystretch}{1.4}
\usepackage{makecell}
\usepackage{relsize}
\usepackage{subcaption}

\usepackage[most]{tcolorbox}
\newtcolorbox{simplebox}[2][]{enhanced,
    minipage boxed title*,
    sharp corners,
    colback=white,
    colbacktitle=white,
    coltitle=black,
    boxrule=1pt,
    left=5mm,
    right=5mm,
    bottom=2mm,
    top=4mm,
    boxed title style={colframe=white},
    attach boxed title to top center={yshift=-3mm},
    center title,
    title=#2,#1}

\usepackage[hidelinks,bookmarks=false]{hyperref}
\usepackage{cleveref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mdframed}
\usepackage{enumitem}
\usepackage{natbib}
	
\usepackage{pdflscape}

\newcommand{\smallish}{\fontsize{6.15pt}{20pt}\selectfont}
\newcommand{\tinyish}{\fontsize{6.4pt}{6.15pt}\selectfont}
\newcommand\circlenum[1]{\raisebox{1.2pt}{\textcircled{\hspace{0.35pt}\scriptsize{\raisebox{-.4pt}{#1}}}}}

\definecolor{todo}{HTML}{00a3cc}
\newcommand{\todo}[1] {{\bfseries \color{todo} #1}}
\newcommand{\caro}[1] {\todo{[Caro: #1]} } 
% hide todos:
% \newcommand{\todo}[1] {}
% \newcommand{\caro}[1] {} 


%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{How to Analyze Build Logs%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
\subtitle{A~Systematic Mapping and Comparative Study of Chunk~Retrieval~Techniques}

%\titlerunning{Short form of title}        % if too long for running head

\author{Carolin~Brandt        \and
        Moritz~Beller %etc.
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{Carolin Brandt \at
              Delft University of Technology \\
              % Tel.: +123-45-678910\\
              % Fax: +123-45-678910\\
              \email{c.e.brandt@tudelft.nl}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           \and
           Moritz Beller \at
              Facebook \\
              \email{moritz.beller@gmail.com}
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
Build logs are the textual by-products that automatic software build processes
create, often as part of their Continuous Integration (CI)
pipeline.
Build logs can serve as a paramount source of
information for developers to understand and debug dependency, compilation, or
test failures.
Manually
extracting the important chunks of information in a build log, though, is akin to
finding a needle in a haystack.
Recently, practitioners and researchers have started attempts to partly
automate
this time-consuming activity, to not only ease
developers' debugging task but also to enable a novel class of
automated onward processing applications.
In this paper, we give a systematic overview of the emerging field of build log analysis.
In a broad literature mapping, we first survey and categorize the described methods to
extract information chunks from build logs.
In the second part of the paper, we then develop prototypical implementations of three promising
techniques to analyze build logs, namely program synthesis
by example, textual similarity, and keyword search.
Finally, we evaluate the techniques in an empirical study on
almost 800 build logs 
from a wide range of different projects and programming languages.
Our findings show that none of the three techniques in general outperforms
the others, but rather, that each technique has its respective strengths
and weaknesses.
We discuss under which circumstances each technique performs best
and provide a recommendation scheme for when developers or researchers might 
use which technique.
We close with a roadmap of important open problems in the field of
build log analysis.
\keywords{Continuous Integration \and Build Log Analysis \and Chunk Retrieval}
\end{abstract}

\lstset{
  morekeywords={},
	basicstyle=\ttfamily\scriptsize,
  postbreak=\mbox{\textcolor{blue}{$\hookrightarrow$}\space},
  showspaces=false,
  showstringspaces=false,
  stringstyle=\color{Plum},
	frame=single,
  extendedchars=false,
  texcl=false,
  aboveskip=\baselineskip,
  belowskip=0pt
}

\section{Introduction}
\label{sec:introduction}
In the past two decades, Continuous Integration (CI) has become a
ubiquitous best practice to streamline the build process of software
projects~\citep{hilton2016usage,beller2017oops,staahl2014modeling}.
Build logs are a textual by-product that these automatic software
builds create.
As a treasure trove of information~\citep{meyer},
build logs contain trace outputs of not only the elementary steps to
``make'' a project---such as retrieving and resolving dependencies and
compiling---but also about ancillary quality assurance steps---such as
testing the software and running automated static analysis
tools~\citep{beller2017oops}.
The
contents and format of build logs can vary highly depending on which tools
are involved in the build process, how they are
configured, and as projects and build tools evolve over
time~\citep{staahl2014modeling}.
Typically, though, they are at least a semi-structured and
somewhat stable journal of the commands executed during the build and
their results.

When a build or one of its steps fails, developers typically scavenge
the logs for information related to the source of the error
~\citep{vassallo2019every}.
This manual activity is tedious and prone to
errors~\citep{santolucito2018statically}.
As a single build log can be
over 50 megabytes large~\citep{beller2017oops} \todo{check, reviewer comment}, finding the small chunk
of information linked to the actual error, is akin to
searching for a needle in a haystack.
The large number of unrelated, but ambiguous
warnings which many build
logs regularly contain further compounds this problem.
In addition to helping
understand and fix build failures, the information richness of build
logs enables a plethora of other applications: with the help of build
logs, we can better understand and categorize CI failures and compile
errors~\citep{islam2017insights,seo2014programmers}, differentiate
testing practices~\citep{orellana2017differences,vassallo2017a-tale},
train classifiers to do ``predictive CI'' on the build or test outcome
and
duration~\citep{ni2017cost,bisong2017built,haghighatkhah2018test,
machalica2019predictive},
or investigate the role of automated static analysis tools (ASATs)
in CI
builds~\citep{zampetti2017open}.

However, manual analysis of build logs does not
scale to the number of build logs and amount of information that such
sophisticated applications require.
Moreover, being able to
automatically extract relevant pieces of information from build logs
can help
developers debug a broken build more quickly without having to browse
the entire build log.

\begin{figure*}[htb]
	\centering
	\includegraphics[width=\textwidth, trim={1.2cm 10.5cm 1.2cm 0cm},
	clip]{img/overview.pdf}
	\caption{Research design.}
	\label{fig:overview}
\end{figure*}

Retrospectively analyzing the build logs is not the only solution to
the problem---an alternate would be to integrate live plugins into the
build process such as the JUnit
plugin for Jenkins\footnote{https://plugins.jenkins.io/junit/} or
Deflaker~\citep{bell2018deflaker}.

Their advantage is that they can precisely report the information which
is required.
However, such plugins are not available for all build tools, they are
non-trivial to implement, and
do not enable the analysis of historical builds.
When using such a plugin, one has to decide
beforehand which information should be reported, whereas
build log analysis enables us to leverage all information
present in the build logs.
Moreover, build log analysis is the more general solution.
While custom plugins serve an important niche, they do not make build
log analysis redundant.

Despite its central role in helping developers and in enabling
novel, onward processing applications,
the process of how to
automatically
analyze build logs has thus far not been systematically
overviewed as a research topic, bringing us to the first
research question:
\begin{simplebox}[minipage boxed title*=-5cm]{\textbf{Research Question
1}}
What is the state-of-research to analyze build logs?
\end{simplebox}

To answer this question, we start the investigation in this
paper---pictured in
\Cref{fig:overview}---with an
extensive literature mapping (step \circlenum{1}).
Researchers describe various strategies to
analyze build logs, among which are developing
custom parsers, using regular expressions, doing manual inspections,
searching for keywords, or performing natural language processing.

However, the mapping identifies that there is currently no guidance on when
to use which of these approaches.
Unfortunately, it also shows that
less than half of the 61 works which rely on build log analysis
describe it in sufficient detail and that
the few available implementations are all based on either a custom parser
or regular expressions.
These two approaches are problematic, as their creation
and maintenance is manual and expensive, does not generalize across build
logs from different build tools, and is prone to errors,
as the experience of TravisTorrent has
shown~\citep{beller2017travistorrent}.
In fact, some even ``strongly discourage'' the creation of such
parsers~\citep{urli2018design}.
In summary, the outcome of our literature mapping shows the need for a new
generation of automated techniques to analyze build logs, which are
openly available and documented in detail, together with
guidance on when to use which technique.

To fill this gap, we created three novel, prototypical implementations
to automatically analyze build logs (step \circlenum{2}) by retrieving
a substring (chunk) from the build log.
Our
implementations are based on widely described approaches
from the literature mapping and as such, are meant to serve as
the ``smallest common denominator,'' with as little tuning and
optimization as possible to stand prototypical for the ideas
they represent:
\begin{itemize}
	\item \emph{Programming by Example (PBE)}, a technique
	able to
	automatically synthesize regular expressions,
	\item a technique based on information
	retrieval, which we call \emph{Common Text Similarity (CTS)},
	\item inspired by the manual ad-hoc search for key parts in the
	log, a
	technique employing \emph{Keyword Search (KWS)}.
\end{itemize}

These three approaches have
different strengths and weaknesses.
As a consequence,
in the second research question we ask:

\begin{simplebox}[minipage boxed title*=-5cm]{\textbf{Research Question
2}}
How do build log analysis techniques compare?
\end{simplebox}

To answer this question
we conduct an empirical study assessing the performance
of these chunk retrieval techniques in
real-world projects under a variety of conditions (step \circlenum{3}).
The study is based on the \emph{LogChunks} data
set~\citep{brandt2020logchunks}, which encompasses 797 build logs from
80 popular open-source projects and a broad range of different
build tools and programming languages.
Our results show that all of the three techniques
clearly surpass a random baseline, but have different strengths and
weaknesses.
There is no technique that in general outperforms
the others.
PBE has the best average precision of 95\%, while KWS has the highest
recall with 70\% on average.
However, CTS shows the best F$_{1}$-score
of 51\%.

Due to this ambiguity, we develop a set of guidelines
on how to choose the most suitable build log analysis
technique for the task at hand (step \circlenum{4}).
We recommend PBE for use cases where the desired information is always
represented in the same structural way and high confidence in
precision and recall of the chunk retrieval is required.
The results PBE produces are suited for automatic onward processing.
CTS is
well-suited when the representation of the desired information varies
slightly and the output of the chunk retrieval is further processed by
a human.
In cases where the textual representation of the desired
information in the log is unpredictable or varies greatly, KWS is
the best choice.
However, its low precision---it extracts a
context of multiple lines around a found keyword---makes it generally
unsuited for automatic onward processing.
Instead, KWS requires a human
to further inspect and interpret the output.
A hybrid solution combining the three techniques might be the best
all-round solution.
Finally, we give a roadmap to guide future research in the field
of build log analysis.
Our implementation, experiment and analysis are documented
in our replication package~\citep{brandt2020chunk-replication}.

In short, this article contributes
\begin{itemize}
\item the first systematic mapping of the emerging field of build log
analysis.
\item three prototypical open-sourced implementations of
promising chunk retrieval techniques.
\item a large empirical study comparing the three techniques.
\end{itemize}

\begin{figure}[tb]
	\centering
	\includegraphics[width=0.7\columnwidth, clip]{img/lit_mapping.pdf}
	\caption{Literature selection (following Petersen et
	al.~\citep{petersen2015guidelines}).}
	\label{fig:lit-mapping}
\end{figure}

\section{Systematic Literature Mapping}
\label{sec:mapping}
% tense: past

In this section, we report on our systematic mapping study of how
researchers analyze build logs.
We start with a differentiation of build log analysis
from the older field of system log analysis.
% ref to fig 2 so it can be here and above fig 3
Then, we describe the methodology (\Cref{fig:lit-mapping})
and results of our systematic mapping study following the
guidelines by Petersen et
al.~\citep{petersen2008systematic,petersen2015guidelines}.
% We close by summarizing how build logs are commonly
% analyzed in literature and discuss the implications of these findings.

\lstset{
  morekeywords={INFO, WARN, 2008, 11, 09},
  alsoletter=-20819,
  keywordstyle=\bfseries\color{Plum},
  escapeinside=**
}
\begin{figure}[b]
  \centering
  \lstinputlisting[breaklines=true]{listings/syslog.txt}
  \caption{System log excerpt (adapted from He et
  al.~\citep{he2017towards}).}
  \label{lst:system-log}
\end{figure}

\subsection{Distinction from System Log Analysis}
\label{sec:system-log-analysis}
% tense: present

A field related to build log analysis is the analysis of system logs.
These system logs are produced during the runtime of a system, while
build logs are produced during the runtime of the build of the system.
In terms of the contents of the logs, the main difference is that system
logs are fundamentally structured
through events.
Each line in a log file represents one event with a
set of fields: timestamp, verbosity level, and raw message
content~\citep{he2017towards}.
\Cref{lst:system-log} depicts two
exemplary lines from such a system log.

Before system logs can be analyzed, the raw log data needs to be
abstracted into a structured
events list~\citep{el-masri2020a-systematic}.
The first step of this log abstraction is to define or discover
log message templates, e.g. by
separating the constant and variable parts within a log
message~\citep{nagappan2010abstracting,he2017towards}.
In the second step of the log abstraction, the log
messages are matched to the templates, unifying messages with
identical constant parts and varying parameters under the same event.
The output is an
ordered list of timed events and their corresponding parameter
values~\citep{he2016evaluation}.
This structured log can serve as
input to log analysis approaches based on, e.g.,
machine learning and data mining processes.
Apart
from a large body of research on system logs, an equally mature amount
of commercial-grade and
open source tooling exists to analyze system logs---such as, for
example, Elasticsearch, Logstash and Kibana (the so-called ELK
stack)~\citep{sanjappa2017analysis,bajer2017building}.

Some of the techniques developed to parse system logs can operate
on build logs as well, such as approaches to retrieve the
values of variable parts in a log message, e.g.\, by using regular
expressions~\citep{nagappan2010abstracting,xu2009detecting}.
% This is the easiest example matching here
% amar et al are not in our literature mapping
However, the number of
system logs generated mandates that scalability and efficiency play a
much more important role than for build logs.
In addition, almost all of the techniques developed to analyze system logs
leverage their inherent event structure.
Build logs generally lack this
structure and thus require different bespoke approaches.

\subsection{Literature Selection}
\label{sec:litsel}
% tense: past

The first step in a systematic literature mapping is to
establish which works one wants to cover and how to find
them~\citep{kitchenham2009systematic}.
Since the field of build log analysis is relatively young, we
mapped as broad a population of scientific material as possible.
We did not limit the sources to specific Software Engineering
venues~\citep{petersen2015guidelines}, but included all sources
monitored by Google Scholar, including Bachelor's theses, Master's
theses, and academic slide decks.
We use Google Scholar as opposed to Scopus or
other databases because it is the search engine for scholarly material
with
the broadest and most current index, including preprints.

\Cref{fig:lit-mapping} gives an overview of the literature selection
process.
An initial Google Scholar search for ``build log''
returned more than 2.2 million results on the 15th of March 2020, many
of which stem
from unrelated fields such as construction or wood working.
We refined the search criteria to exclude such obvious
non-related fields and to exclude works on system
logs (see \Cref{sec:system-log-analysis}).
This left us with 1,552 search results.
% Our replication package documents the full search
% queries~\citep{brandt2020chunk-replication}.
To make handling so many search results feasible, we employed a
three-pass filtering strategy:

First, we filtered works based on (a) their title and (b) the
contextual information Google Scholar displayed on its search results
page.
We included works at large that seemed to bear a resemblance to software
engineering and building software.
Following Kitchenham's protocol~\citep{kitchenham2009systematic},
we excluded
works written in a language unintelligible to the authors
(i.e., not English or German), which disregarded fewer than 1\% of search
hits.
% 'four' not clear and it gets lost that there are multiple search queries
% therefore I'll cut this part completely:
% We then unified
% the results of the four search queries based on the link as the
% identifying element, removing 60 works that appeared in more than
% one search query.
We removed duplicates based on the title and were
left with 256 works (16\% of the original search results).

If a work referenced a new work in the context
of build log analysis, we added the new one to the literature set.
This added five works (eight before
duplicate elimination), leading to a total of 261
works.

Second, we performed a finer filter phase by (1) reading abstracts,
and (2) downloading the
full text of the works and searching it
for the occurrence of the term ``log.''
If a work showed traces of
working with build logs, we passed it onto the final filter phase.
In total, we
were left with 87 works after the second filtering phase.

Third, since we had been careful to include rather than exclude borderline
works, a deeper evaluation excluded another 15 works from the 87
works left after the second filter phase.
Multiple works were extensions of others.
Of these, we only considered the earliest work which reported on
the analyzing of build logs.
This excluded another 11 works.
In sum, we included and classified 61 works in detail.

\begin{figure}[tbp]
		\centering
		\includegraphics[width=0.8\textwidth,
		clip]{img/lit-sur/years.pdf}
		\caption{Number of works analyzing build logs per year.}
		\label{fig:lit-years}
\end{figure}

\paragraph{Descriptive Study Statistics}
After the third filtering phase, we finally included 61 works in the
literature mapping.
The majority of these are scientific articles (81\%), of which all but
three have been peer-reviewed and published (94\%).
The remaining works are theses (14\%), two books, and one slide deck.

\Cref{fig:lit-years} depicts the year of appearance of these
works.
Although we applied no filtering by year in our initial search queries
(see \Cref{sec:litsel}), the earliest work in our literature corpus
stems only from 2001.
There are various scattered works in the years between then and 2016,
when the field of build log analysis started to grow rapidly.

\subsection{Literature Classification}
Having trimmed down the set of works by 95\%, we investigated the
remaining ones in-depth.
Our aim was to characterize how researchers work with build logs.

Specifically, we were interested in
\begin{itemize}
  \item what information the researchers retrieve from build logs and
  what they use this information for (\textbf{RQ1.1}),
  \item which techniques they employ to process the build logs,
  in how much detail the techniques are described,
	and whether their implemented tools are published
  (\textbf{RQ1.2}),
  \item which kinds of build logs the researchers analyze
	and the origin of these logs (\textbf{RQ1.3}).
\end{itemize}

This lead us to the following research questions:
\begin{simplebox}[attach boxed title to top center={yshift=-6mm}]
{\textbf{RQ1:} What is the state-of-research to analyze build logs?}
\begin{itemize}[leftmargin=1cm]
  \item[\textbf{RQ1.1:}] What information is targeted in build logs?
  \item[\textbf{RQ1.2:}] Which analysis techniques exist?
  \item[\textbf{RQ1.3:}] Which kinds of build logs are subject to
  analysis?
\end{itemize}
\end{simplebox}

To support the data extraction we created a template which we filled out
for each of the 61 works.
The template contained questions to answer RQ 1.1 through RQ 1.3,
for example ``Is the technique explained in detail?'' and
``What is the source of the build logs?''
As recommended by Petersen et
al.~\citep{petersen2015guidelines}, we started the mapping with a
pilot:
both authors extracted data from the same five works and held a
consensus meeting to unify their understanding
of the data extraction template.
We divided the remaining 56 works evenly between the two authors
and inspected each work as closely as necessary to fill out the
template
with confidence,
starting from occurrences of ``log'' or ``build'' within the full
text of the work.

For questions that did not require a boolean answer, we allowed assigning
multiple tags or categories as the first part (``preparation phase'')
of a virtual open card sorting~\citep{zimmermann2016card}.
An example of this is the question ``What is the source of the build
log?,''
where we assigned tags such as ``Travis CI,''
``Industrial,'' ``Google,'' or
``Self-built.''
Once we had classified all works, we went into the second phase of
card sorting
(``execution phase''), in which we grouped tags
together and found synonyms.
% the following adds to the example but is maybe unneeded / too much
For instance, in the case of the build log source we grouped
``Travis'' and ``Travis CI'' together and assigned ``Industrial'' to
all works that used build logs from a company.
The replication package contains the data extraction template and all
results of this step~\citep{brandt2020chunk-replication}.


\subsection{Results}
% tense: past

\label{sec:litsurresults}

In this section, we present the results of the analysis phase along RQs 1.1 through
1.3.
\todo{if defining the RQs went up in the last section, rephrase the rqs here too}

\subsubsection{What information is targeted in build logs (RQ 1.1)?}
\label{sec:rq11}
\begin{figure}
\centering
\begin{minipage}[b]{0.45\textwidth}
		\subcaptionbox{Information targeted in build log analysis.
		\label{fig:litsur:info_target}}
		{\includegraphics[width=\textwidth,
    trim={2cm 0 0 0},
		clip]{img/lit-sur/info_target.pdf}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
		\subcaptionbox{Class of information analyzed
		\label{fig:litsur:kind}}
		{\includegraphics[width=\textwidth,
    clip]{img/lit-sur/kind.pdf}}
    \vskip\baselineskip
    \subcaptionbox{Purpose of build log analysis.
		\label{fig:litsur:use}}
		{\includegraphics[width=\columnwidth,
		clip]{img/lit-sur/use.pdf}}
\end{minipage}

\caption{Multi-label categorization of the 61 relevant works.}
\label{fig:litsur_r}
\end{figure}

In this section, we describe what sort of information the works in
our mapping usually retrieve from build logs.
\Cref{fig:litsur_r} depicts these results.

\Cref{fig:litsur:info_target} gives an overview over the frequency of
the targeted information.
The precise information targeted in the build logs varied from work
to work, with a relatively long tail.
Most prominent was the search for errors (39\%), followed by extracting
the executed or the failing
tests (both 11\%).
There were also numerous works which targeted more specialized
information, such as the environments used during the
build~\citep{zolfagharinia2017not}, packages
installed~\citep{selberg2012use}, or hints to source files which
relate to build failures~\citep{ren2018automated}.

\Cref{fig:litsur:kind} depicts that there are two widely used ways to
extract information from build logs: either chunks (53\%) or
classifications (41\%).
Uses for chunks were extracting compilation errors,
test failures, or the duration of build
tasks~\citep{clemencic2014new,zhang2016android}.
Examples for
classifications were distinguishing failures caused by developers from
failures caused by infrastructure errors~\citep{lindqvist2019detection}
or determining whether projects use automated static analysis
tools~\citep{kavaler2019tool}.
Categorization is the more abstract representation, chunks
contain more information and could be turned into a categorization.

\Cref{fig:litsur:use} shows the aim for the
investigated works to analyze build logs, and the result also explains
the prevalence of classification in \Cref{fig:litsur:kind}.
The majority of works (46\%) used the outcome of their build log
analysis to drive further research---and it is science's aim to
classify and abstract.
Seo et al.~\citep{seo2014programmers}, for example,
categorized the types of compile errors encountered at
Google.
Other works (10\%) used the gained insights to give feedback to
developers.
For 8\% of the works was the retrieved information the input
to a next automatic onward processing step:
works chose
tests that should be part of a reduced, but still effective test
suite~\citep{shi2018evaluating} or filled     a data structure with
build information, an information source for further tools aiming
to fix a failing build~\citep{vassallo2018un-break}.

\begin{landscape}
\addtolength{\tabcolsep}{-5pt}
\begin{table*}[tbhp]
\tinyish
\centering
\caption{Overview of build log analysis techniques.}
\begin{tabularx}{1.5\textwidth}{lXl}

\toprule
Name			     & Sources	& Frequency	  \\
\midrule

{Parser} &
\citep{vassallo2018un-break} \citep{zhang2016android} \citep{seo2014programmers} \citep{hassan2019tackling} \citep{hassan2017automatic} \citep{chromy2007integration} \citep{mesbah2019deepdelta} \citep{wen2018blimp} \citep{kwon2018prioritizing} \citep{adams2007design} \citep{rahman2018impact} \citep{brandyberry2006continuous} \citep{tomassi2019bugswarm} \citep{ren2018automated} \citep{vassallo2019automated} \citep{cavalcanti2019impact} \citep{sippola2013qt} \citep{felipe2012towards} \citep{shi2018evaluating} \citep{urli2018design} \citep{selberg2012use}
 &
\includegraphics[width=0.35\columnwidth]{img/lit-sur/techniques-no-guidelines-cropped_21.pdf}
\\

{Regular expression} &
\citep{beller2017oops} \citep{hassan2017change} \citep{macho2018automatically} \citep{vassallo2017a-tale} \citep{lou2019history} \citep{hassan2017automatic} \citep{rott2019empirische} \citep{zampetti2019study} \citep{zhao2018comparing} \citep{rausch2017empirical} \citep{ghaleb2019studying} \citep{zampetti2017open} \citep{zhang2019large} \citep{kavaler2019tool} \citep{morris2010experience}
 &
\includegraphics[width=0.35\columnwidth]{img/lit-sur/techniques-no-guidelines-cropped_15.pdf}
\\

{Manual inspection} &
{
\citep{sulir2016quantitative,hassan2017automatic,bouabana2019theory,barinov2017applying,silva2018build,ghaleb2019empirical,marcozzi2019systematic,hukkanen2015adopting,rausch2017empirical,hassan2017mining,zolfagharinia2017not,cassee2019impact}
} &
\includegraphics[width=0.35\columnwidth]{img/lit-sur/techniques-no-guidelines-cropped_12.pdf}
\\

{Machine Learning} &
{
\citep{hassan2017change,lou2019history,lindqvist2019detection,ren2018automated,schulz2017active}
} &
\includegraphics[width=0.35\columnwidth]{img/lit-sur/techniques-no-guidelines-cropped_5.pdf}
\\

{Natural Language Processing}	&
{
\citep{hassan2017change,lou2019history,schulz2017active}
} &
\includegraphics[width=0.35\columnwidth]{img/lit-sur/techniques-no-guidelines-cropped_3.pdf}
\\

{Information Retrieval} &
{
\citep{hassan2017change,lindqvist2019detection,ren2018automated}
} &
\includegraphics[width=0.35\columnwidth]{img/lit-sur/techniques-no-guidelines-cropped_3.pdf}
\\

{Analysis} &
{
\citep{sulir2016quantitative,haghighatkhah2018test,durieux2019critical}
} &
\includegraphics[width=0.35\columnwidth]{img/lit-sur/techniques-no-guidelines-cropped_3.pdf}
\\

{Keyword Search} &
{
\citep{brandyberry2006continuous,zhang2019large,kavaler2019tool}
} &
\includegraphics[width=0.35\columnwidth]{img/lit-sur/techniques-no-guidelines-cropped_3.pdf}
\\

{Scan} &
{
\citep{clemencic2014new,hibbard2001visualization}
} &
\includegraphics[width=0.35\columnwidth]{img/lit-sur/techniques-no-guidelines-cropped_2.pdf}
\\

{Other} &
{
\citep{zhang2016android,hassan2017change,lou2019history,silva2018build,ren2018automated,schulz2017active}
} &
\todo{why is this not in the graph?, find bar graphic}
\\

{None identified} &
{
\citep{macho2017preventing,felipe2012towards,orellana2017differences,madeyski2017continuous,zhao2017impact,santolucito2018statically,makihara2018multi,mcintosh2012evolution,gallaba2018noise,matthies2016scrumlint}
} &
\includegraphics[width=0.35\columnwidth]{img/lit-sur/techniques-no-guidelines-cropped_10.pdf}
\\

\specialrule{\heavyrulewidth}{0pt}{2pt}

\end{tabularx}
\label{tab:litsur:techniques}
\end{table*}
\addtolength{\tabcolsep}{5pt}
\end{landscape}

\subsubsection{Which analysis techniques exist (RQ 1.2)?}

Of the 61 works we inspected, 43 mentioned how they analyzed
build logs.
However, only 16 of these described their technique in detail.
We summarize the techniques that the works
described for analyzing build logs in \Cref{tab:litsur:techniques}.

The most mentioned technique (34\%) was using a parser.
In this tag, we also
included
rather imprecise statements like ``we parse the build
logs''~\citep{rahman2018impact}.
For such imprecise statements it was often not clear if they described
a complex parser (including a tokenizer, lexer, etc.) or referred to a
simpler parsing tool based on regular expressions.
If we saw evidence for the latter, we assigned the tag
``Regular Expression'' instead of the tag ``Parser.''
25\% of the works described the use of regular expressions and 20\%
inspected the logs manually.
For example,
Seo et al.~\citep{seo2014programmers} developed a custom
parser to classify error messages, while Vassallo et
al.~\citep{vassallo2017a-tale} analyzed build logs with regular
expressions.
Ghaleb et al.~\citep{ghaleb2019studying} used a compound approach.
They started with a manual categorization of build logs and selected
search strings that identify the target category.
Based on the search strings they created a script that automatically
classifies the remaining logs.
Another 8\% of the investigated works
employed machine learning such as natural language
processing or information retrieval techniques.
The rather general tags ``analysis'' and ``scan'' in
\Cref{tab:litsur:techniques}, stem---similar to the imprecise
statements about parsing---from the
words authors used to describe their techniques.
%such as
%``automatically analyzed [the build logs]''.


Only 25\% of the works stated that they make their
implementation available for reuse.
We saw some reuse of the few available build log analysis tools.
Five works employed the tools used to create
\emph{TravisTorrent}~\citep{beller2017oops,
orellana2017differences,zhao2018comparing} or
an updated version of them~\citep{rott2019empirische,
shi2018evaluating}, two used the
\emph{Maven Log Analyzer}~\citep{macho2018automatically,gallaba2018noise},
and another two
\emph{MAKAO}~\citep{wen2018blimp,adams2007design}.

\subsubsection{Which kinds of build logs are subject to
  analysis (RQ 1.3)?}
As the structure of build logs can change greatly depending on which
build tools output to it~\citep{staahl2014modeling},
we were interested in which build tools are
investigated in the literature.
\Cref{fig:litsur:log_producer} shows the distribution of
build tools whose logs were analyzed by the works in our mapping.
Of the works, 30\% analyzed
Maven\footnote{https://maven.apache.org/},
16\% analyzed Gradle\footnote{https://gradle.org/},
and 10\% analyzed Ant build logs\footnote{https://ant.apache.org/}.

A large fraction of the works analyze build logs
from a small number of build tools.
In most cases, this is because the format of build logs
changes and
therefore ``parsers must be specialized to each build and test
framework''~\citep{tomassi2019bugswarm}.
Sometimes, aspects separate from the build log analysis motivate
the limitation to a specific build tool.
For example, Shi et al.
focussed on Maven
build logs because they also chose the PIT tool~\citep{coles2016pit}
to calculate coverage
and mutation score~\citep{shi2018evaluating}.
PIT is only available as a Maven plugin.
Only 7\% of the proposed techniques claimed to be
build tool-agnostic, while
several described that they cover multiple build tools.
The majority of works (42\%) analyzed logs produced by a build tool
that no second work investigated.

In 38\% of cases, the works collected build logs from Travis
CI\footnote{https://travis-ci.org};
18\% of the works analyzed logs from TravisTorrent; and 8\% included
build logs from industrial projects.
The number of logs analyzed varied greatly between the works
and we were not always able to
extract it confidently.
23 of the works analyzed more than a thousand build logs, while one
investigated up to 122 million logs.
Of the mapped works, 31\% claim to have published their data to
enable further research.
However, we found no clear reuse of build log data sets with the
exception of TravisTorrent~\citep{beller2017travistorrent}, which
Vassallo et al.~\citep{vassallo2017a-tale},
Ghaleb et al.~\citep{ghaleb2019studying},
and several others have studied~\citep{hassan2017change,
hassan2019tackling,hassan2017automatic,orellana2017differences,
haghighatkhah2018test,madeyski2017continuous,ghaleb2019empirical,
santolucito2018statically,gallaba2018noise,shi2018evaluating}.

\begin{figure}[tbp]
		\centering
		\includegraphics[width=0.8\textwidth, trim={1.5cm 0.4cm
		1.5cm 0.5cm},
		clip]{img/lit-sur/log_producer_annotated.pdf}
		\caption{Frequency of supported build tools.}
		\label{fig:litsur:log_producer}
\end{figure}

\subsection{Discussion}
\label{sec:lit-sur:discussion}
% tense: present

% ----- descriptive / general -----
Our literature mapping shows that \emph{there is a rapidly growing body
of works using build log analysis,}
with 61 works in different venues making use of it for a variety of
purposes.
We saw that various researchers analyze build logs, with the
logs often being the only available source for the
information they need for their studies~\citep{ren2018automated,
seo2014programmers,beller2017oops,zampetti2017open,rausch2017empirical}.

% ----- information RQ 1.1 -----
In regards to the information that is targeted, \emph{most build log
analysis approaches aim to retrieve the reason a build failed} by
targeting ``errors'' or ``failing tests.''
% which is what we also do in LogChunks
\emph{The majority of approaches retrieves specific chunks of
text from the logs.} Chunks are the overarching concept to
the almost equally much used categorization: one can deduce a
categorization from chunks, but not vice versa.
\todo{catedorization was called classification before}

% ----- technique RQ 1.2 -----
\emph{Most log analysis is automated.} Automated approaches are
necessary because many studies target a large number of builds.
Works
that explicitly mention manual analysis of the logs, mostly
restrict themselves to few logs or take a ``representative sample to
make manual analysis feasible''~\citep{zolfagharinia2017not}.
We observed \emph{very little reuse of build log analysis tools},
even though works reported that
\emph{developing automated analysis tools requires a lot of effort}.
Urli et al.~\citep{urli2018design} strongly discouraged from parsing
build logs for information as it is ``too error prone,'' while other
works pointed to the effort of developing a custom tool.
We know from other research lines that regular expressions, the second
most popular way to analyze log files (\Cref{tab:litsur:techniques}),
are tedious to
maintain~\citep{michael2019regexes}.
In addition, we rarely saw works justifying why they use a certain
technique to analyze build logs as there is a general \emph{lack of
guidance on when to use which technique.}

% ----- kinds of build logs RQ1.3 -----
Several works noted the variety in log formats for different build
tools, which requires customizing build log techniques for
every supported build tool.
\Cref{fig:litsur:log_producer} shows that there are only a handful of
build tools whose build logs more than one study analyzed, mainly in
the Java ecosystem (area \circlenum{A}).
Many works target a unique build tool that did not appear in any other
study (area \circlenum{B}).
We follow that \emph{many seldomly-investigated
environments would benefit from the
existence of a more generic solution.}

% \caro{can we soften this to lead into the new arguments in the following section?}
% The literature mapping shows the need for
% build log
% analysis techniques that are \emph{automated},
% and can be \emph{configured} to target an \emph{arbitrary build tool}
% with \emph{little effort} from the user.


\section{Chunk Retrieval Techniques}
\label{sec:techniques}

Based on our observations from the literature mapping, we believe that
researchers and developers would benefit from pre-made
build log analysis tools they
can re-use and configure to fit the kind of build logs and information
they are targeting.
Such \emph{configurable build log analysis tools} would save high effort
of developing custom tools we observed, and, when openly available,
improve the reproducibility of studies that mine information from
CI build logs.

To make a first step, this paper investigates three different approaches
to realize configurable build log analysis.
One of our central requirements is, that a future user should be able
to adapt the analysis with little effort to their use case.

To enable configurable build log analysis, we evaluate the following
techniques:
\begin{itemize}
  \item \textbf{\emph{Programming by Example (PBE)}}, which learns
  regular expressions based on few user-given log excerpts.
  We rely on the PROSE
  Framework\footnote{https://microsoft.github.io/prose/},
  which also provides the auto-filling for
  cells in Microsoft Excel.

  \item Inspired by information retrieval, a
  \textbf{\emph{Common Text Similarity (CTS)}} approach that finds log
  parts with similar vocabulary than that of user-given log excerpts.

  \item A \textbf{\emph{Keyword Search (KWS)}} that searches for terms
  provided by the user.
\end{itemize}

All these three techniques retrieve \emph{chunks} of text from an
analyzed build log, which is why we will further call them
\emph{Chunk Retrieval Techniques}.

In this section, we introduce these three techniques with the help of an
example, discuss their realization in our prototypes in more detail
and discuss the option of using machine learning to create a configurable
build log analysis approach.

\subsection{Concrete Example}
\label{sec:crt-example}

\lstset{escapeinside=//}
\begin{figure}[tbp]
  \centering
\begin{subfigure}[tbp]{\columnwidth}
  \begin{lstlisting}[breaklines=true,frame=tlr]
FAILURE: Build failed with an exception.

* What went wrong:
  \end{lstlisting}
  \vspace{-\baselineskip}
  \begin{lstlisting}[backgroundcolor=\color{Cerulean!60},breaklines=true,frame=rl]
Could not determine the dependencies of task ':app:jacocoTestDebugReport'.
> Task with path 'testDebug' not found in project ':app'.
  \end{lstlisting}
  \vspace{-\baselineskip}
  \begin{lstlisting}[breaklines=true,frame=blr]

* Try:
  \end{lstlisting}
\end{subfigure}\hspace{\fill}
\begin{subfigure}[tbp]{\columnwidth}
  \centering
  \begin{lstlisting}[breaklines=true,frame=tlr]
FAILURE: Build failed with an exception.

* What went wrong:
  \end{lstlisting}
  \vspace{-\baselineskip}
  \lstinputlisting[backgroundcolor=\color{Cerulean!60},breaklines=true,frame=rl]{listings/chunk1.txt}
  \vspace{-\baselineskip}
  \begin{lstlisting}[breaklines=true,frame=blr]

* Try:
  \end{lstlisting}
    \caption{Two examples of log chunks explaining why an Android
  build failed.}
  \label{lst:chunk-example}
\end{subfigure}

\begin{subfigure}[tbp]{\columnwidth}
  \begin{lstlisting}[breaklines=true,frame=tlr]
=== RUN   TestSeparator
  \end{lstlisting}
  \vspace{-\baselineskip}
  \lstinputlisting[backgroundcolor=\color{Cerulean!60},breaklines=true,frame=rl]{listings/chunk2.txt}
  \vspace{-\baselineskip}
  \begin{lstlisting}[breaklines=true,frame=blr]
=== RUN   TestGenerateHTML
  \end{lstlisting}
  \caption{Example of a log chunk showing a linter error.}
  \label{lst:chunk-example-3}
\end{subfigure}

\caption{Exemplary log excerpts.}
\label{lst:logexamples}
\end{figure}

\lstset{language=caml, morekeywords={StartExtraction, RegexPosition,
RegexPair,
  EndExtraction}, keywordstyle=\bfseries\color{black}}
\begin{figure}[tbp]
  \centering
  \lstinputlisting[breaklines=true]{listings/prose.txt}
  \caption{Simplified regular expression produced by PBE when trained
  with examples from \Cref{lst:chunk-example}.}
  \label{lst:prose-program-simplified}
\end{figure}

In this section, we illustrate how the different chunk retrieval
techniques work practically on the example of the build logs
in \Cref{lst:logexamples}.
Marked in blue in \Cref{lst:logexamples}
are the chunks we target.
They contain the information on why the
corresponding build failed.
\Cref{lst:chunk-example} shows two excerpts
from Android build logs.
The chunk to be retrieved starts after a
colon and a line separator and before a capital letter.
It ends
before two line separators.
All these characteristics are consistent
with the two example chunks
from \Cref{lst:chunk-example}.
\Cref{lst:chunk-example-3} is very
different.
For example, it does not have the two extraneous newlines
after the chunk.

When we train PBE with the two examples from \Cref{lst:chunk-example},
it produces a regular expression program similar to the one presented
in
\Cref{lst:prose-program-simplified}.
This shows that the characters structuring the text around the chunk are
crucial for regular expressions to identify a log chunk.
In fact, if we add the third example from
to the training set (which has a
different structural representation),
PBE is not able to synthesize a program anymore and returns
``\texttt{no program found}.''
The chunks from \Cref{lst:chunk-example}
are in the \emph{same structural category}, while the chunk from
\Cref{lst:chunk-example-3} is in a
\emph{different structural category}.


If we train CTS with the example chunks from
\Cref{lst:chunk-example},
CTS will rank the lines from an analyzed build log which are most
similar to the chunks in the training examples.
It will then return the two lines ranked highest, as the chunks in
the training examples have on average 2.0 lines.
In this example,
CTS will have difficulty extracting the lines from
\Cref{lst:chunk-example-3} as there are no words that appear
in both the training and the test chunks.
The same applies if we train with the first chunk from
\Cref{lst:chunk-example} and try to retrieve the second chunk,
as the only shared word is ``task''.

Keywords that appear in or close to the chunk within
the build log configure the technique KWS\@.
For the examples from \Cref{lst:chunk-example} these could be
``FAILURE,'' ``wrong,'' or ``failed.''
The example from \Cref{lst:chunk-example-3} could be found
by searching for ``FAIL.''
If we train KWS with all three examples and the aforementioned
associated keywords, it would search for the three keywords from
\Cref{lst:chunk-example}, as they appear twice---most often within
the training set of three examples.


% \subsection{Characteristics of Chunk Retrieval Techniques}
% \label{sec:crt-characteristics}

% With the term ``chunk retrieval technique,'' we refer to methods that
% automatically extract strings that appear literally in build
% logs, i.e., techniques that do not aggregate, combine, or deduct
% information.
% We call such pieces of information in build logs
% \emph{chunks}.
% Depending on the format of the chunks within the build log we
% separate them into different \emph{structural categories}.
% \todo{is it necessary to introduce strcutural categories here?
% if yes, we should define it and give an example}

% Generally, chunk retrieval techniques
% can also be used to classify build logs, e.g.\, by checking
% which substring was retrieved.
% Therefore, they
% cover the great majority of the types of log analysis we saw in
% \Cref{sec:lit-sur:discussion}.

% The techniques we investigate here do not require a formal lexer and
% parser to analyze the entire structure of build logs, but focus on
% ad hoc extraction of just one specific information piece per
% configuration.

% \todo{incorporate that this is done to adapt the chunk retrieval
% technique for a certain build tool or other kind of information}
% With the term \textit{configuration}, we abstract over the training
% and parametrization that different techniques require in different
% forms.
% A configuration can be explicitly stated or implicitly derived
% by learning through provided examples.
% It is therefore a manual
% specification of which information the chunk retrieval should target.
% It also supplies the necessary information for the technique to
% identify the targeted chunk in a build log.
% Each chunk
% retrieval technique has a specific \textit{granularity}, i.e.,\ the
% smallest piece of text it can return (e.g., a line, or a word).
% % The
% % granularity might be adjustable by configuration.

% \emph{Running a
% chunk retrieval technique} means
% executing a fully configured
% technique to a build log.
% The technique then outputs a (possibly empty) list of substrings of the
% build log text.

\subsection{Choice of Chunk Retrieval Techniques}
\label{sec:choice}

\begin{table*}[htb]
\centering
\caption{Chunk retrieval techniques.}
\begin{tabularx}{\textwidth}{@{}llXll@{}}
\toprule
Name & Acronym & Identification Technique & Granularity &
Configuration \\
\midrule
\makecell[l]{Programming\\by Example} & PBE	 & Regular expression program
& Character   & In/output examples	\\
\makecell[l]{Common Text\\Similarity}	     & CTS     & TF-IDF \& cosine similarity,
expected number of lines & Line        & Output examples	   \\
\makecell[l]{Keyword Search}		     & KWS     & Keywords, expected number of
lines			 & Line        & \makecell[l]{Keywords,\\context length}  \\
\makecell[l]{Random Line\\Retrieval}	     & RLR     & Random sample
& Line	      & Retrieval length	  \\
\bottomrule
\end{tabularx}
\label{tab:techniques}
\end{table*}


For this article, we designed and implemented three automated chunk
retrieval techniques.
Our findings of techniques in the literature
mapping (~\Cref{sec:litsurresults} and \Cref{tab:litsur:techniques} in
particular) inspired the strategies that underlie them.
\Cref{tab:techniques} summarizes the techniques, which we describe here
in detail.

Techniques based on custom parsers (\#1
in \Cref{tab:litsur:techniques}) are inadequate for our comparison,
since their creation is manual and one parser would never generalize
to the wide variety of build logs we are interested in.
\todo{make clearer: to automatically derive them from a few
user-given examples}
The closest
we could come to the second most popular entry, regular expression, is
a technique to automatically derive them: in particular, we implement
a technique based on
\emph{Programming by Example (PBE)}.
It uses regular expressions internally, while simplifying their
development.

From the fields of information retrieval (IR) and natural
language
processing (NLP, see \Cref{tab:litsur:techniques}), we leverage
a widely-used text similarity measure and call this approach \emph{Common
Text Similarity (CTS)}.
The intuition behind it is that the relevant,
failing parts of build logs often look similar to each other, even if
the exact failure reasons might differ.

To work effectively with build logs, many humans anecdotally search
(``grep'') the logs for key phrases such as ``fail.'' Researchers often
resorted to such a strategy when doing manual analysis (\#2
in \Cref{tab:litsur:techniques}), describing it with the word ``scan''
or other paraphrases.
Imitating this ad hoc approach of searching for
specific strings, we automatize it as \emph{Keyword Search (KWS)}.

Other techniques, specifically ones based on deep learning, are
thinkable, but we did not evaluate them in this article.
They typically require lots of training and tuning,
together with a large body of high quality training data.
However, our premise was to ease
the creation of build log analysis.
First having to
label thousands of logs on a project before one can start to
analyze its build logs is therefore infeasible.
Similarly, as a manual
effort, the \emph{LogChunks} data set is not big enough to support the
training of such supervised techniques.
Finding a way to train deep learning techniques in
an unsupervised fashion would rapidly increase their applicability and
appeal to build log analysis.

\todo{Intro section? In this section we discuss the details of our
prototypes that apply the three chosen techniques to build log
analysis.}

\subsection{Programming by Example (PBE)}
The idea behind \emph{Programming by Example} is to automatically
generate a program, by capturing the
intent of the user through examples that the user provides.
Leveraging this approach, we designed a chunk retrieval technique
for build logs.
The \emph{PROSE library}\footnote{https://microsoft.github.io/prose/}
forms the basis for our implementation.
This library builds on the generic program synthesis framework
\emph{FlashMeta}~\citep{polozov2015flashmeta:} and the specialized
text extraction DSL \emph{FlashExtract}~\citep{le2014flashextract:}.

To configure our prototype, the user supplies one or more build logs
together with the excerpt of these logs they would like to be
retrieved.
The PROSE program synthesis then tries to construct a program
consistent with all training
examples~\citep{mitchell1982generalization}.
If PROSE cannot synthesize a program, e.g.,\
because the required regular expression
is too complicated to be synthesized, our prototype returns the
error message ``no program found.''

When our PBE prototype analyzes a build log, it tries to match
the synthesized regular expression program.
It then returns the matched substring.
If there is no match, because the analyzed build log
does not contain the structure that the training examples define,
PBE returns the error message ``no match found.''

\subsection{Common Text Similarity (CTS)}
Text Similarity approaches are widely used to filter unstructured
textual software artifacts~\citep{runeson2007detection,
marcus2005recovery,antoniol2002recovering,mccarey2006recommending}.
These approaches fall into the broader area of information retrieval
techniques, which several of the works from our literature mapping
used to analyze build logs.
Similar to these approaches, we designed a chunk retrieval
technique that
retrieves those lines from a build log which have a similar
vocabulary to the log excerpts the user provides as configuration.

Using a Vector Space
Model~\citep{schutze2008introduction} (VSM), an algebraic model for
representing textual documents, we encode each line of the
user-provided log excerpts as a separate term vector.
The terms that make up the dictionary are simply all words
that appear in the example logs.
We separate words by spaces and
punctuation.
The vectors comprise points counting
how many times a word out of the dictionary appears in the specific
line at hand.

The idea behind a representation in the VSM is that text lines as more
similar if they contain the same words in similar frequencies,
making their term vectors point in a similar direction in vector
space.
We use \emph{cosine similarity}~\citep{korenius2007principal}
to calculate the angle between two term vectors.
The smaller the
angle between term vectors is, the more similar are the corresponding
lines.

To improve the similarity calculation, we prune very often or
very rarely appearing words.
Finally, the
algorithm weighs the vectors using \emph{term frequency-inverse
document frequency (TF-IDF)}, an established technique to give a higher
impact to words that occur in a smaller number of
documents~\citep{lee1997document}.

To analyze a build log, we encode every
line of the build log in VSM.
The algorithm calculates the cosine
similarity to compare each line of the
build log with each line of the search query.
After summing up the
similarities of each build log line to all search query lines, we sort
the build log lines in decreasing similarity.
The average number of
lines in the chunks of the training examples determines how many of
the most similar lines our approach returns as the output of the retrieval
run.

\subsection{Keyword Search (KWS)}
When developers scavenge for a specific piece of information within a
large amount of unstructured information, a first ad hoc approach they
use is to search for related keywords.
To investigate the performance of this approach, we designed a chunk
retrieval technique mimicking this human behavior based on keyword
search.

The user provides a list of search terms to configure KWS.
To analyze a build log, our prototype searches for all
exact occurrences of the search keywords.

To better
compare KWS with the other two techniques,
we also configure it through a set of log excerpts.
From each excerpts, we manually extract a list of keywords that
appear in the
training examples and that we expect to appear
in, or close to, the targeted chunk in a new build log.
Finally, we merge the lists from all examples to build one unified
list of the most commonly occurring keywords for the search with KWS.
As keywords are
often not directly describing the desired information, but rather
appear close to it, KWS also retrieves a context of lines
around the found keyword.
The number of surrounding lines retrieved is
the average number of lines in the chunks of the training examples.

\subsection{Random Line Retrieval (RLR)}
\label{sec:expl-rlr}
To fully comprehend how difficult the task of build log analysis really
is without any a priori knowledge, we include a comparison with a
baseline of
picking lines at random from the build log.
RLR mimics the
situation of guessing blindly which lines are interesting.
The only configuration option for RLR is the number of lines it should
retrieve.
For a fair comparison to the other techniques (whose number of
returned lines is largely determined by the training set),
we let RLR return the average
number of lines in the log excerpts used as configuration.

\subsection{Tool Implementation}
For the comparison study, we implemented the three chunk retrieval
techniques described in this section and a unifying interface.
We implemented PBE in C\# based on the Microsoft PROSE
library\footnote{https://microsoft.github.io/prose/}
and CTS, KWS, and RLR using
R and the {\tt text2vec} library\footnote{http://text2vec.org/}.

\section{Chunk Retrieval Comparison}
\label{sec:study}

\begin{figure*}[tb]
\begin{algorithmic}[1]
\ForEach{$technique \in\{PBE,CTS,KWS,RLR\}$}
 \ForEach{$project  \in \emph{LogChunks}$}
 \Comment{$|\emph{LogChunks}|$=80}
  \For{$i \in\{1,2,3,4,5\}$}
   \State $training\_logs \gets getBuildLogsByTime(project, 0, i)$
   \Comment{Logs
   returned ordered by time of creation.}
   \State $test\_log \gets  getBuildLogByTime(project, i)$
   \State $n \gets |structuralCategories(test\_logs)|$
   \State
   \State $train(technique, training\_logs)$
       \State $retrievedLines \gets run(technique, test\_log)$
       \State $metrics \gets calculateMetrics(retrievedLines,
       desiredLines(test\_log))$
       \State
       \State $saveMetrics(technique, project, n, i, metrics)$

  \EndFor
 \EndFor
 \State $aggregateMetrics(technique)$
\EndFor
\end{algorithmic}

	\caption{Study procedure.}
	\label{fig:study}
\end{figure*}


\caro{To investigate which of the three techniques are feasible to
facilitate configurable build log analysis, we conduct an empirical
comparison on a large dataset of real world log files and excerpts.
}


In the literature mapping, we noted that previous work seemed to choose
a log analysis technique arbitrarily and that is a lack of guidance on
when to use which technique.
% add (see \Cref{sec:lit-sur:discussion}) ?
To address this, in this section, we conduct an empirical study
comparing the three novel chunk retrieval techniques we presented in
\Cref{sec:techniques}.

In particular, we are interested in how many examples a technique
should be trained with (\textbf{RQ2.1}),
how similar the structural representation of the targeted log chunks
has to be (\textbf{RQ2.2}), and how reliable the quality of the
produced retrievals is (\textbf{RQ2.3}).
This leads us to the following research questions:

\begin{simplebox}[minipage boxed title*=-1.5cm,
attach boxed title to top center={yshift=-6mm}]
{\textbf{RQ2:} How do build log analysis techniques compare?}
\begin{itemize}[leftmargin=1.2cm]
  \item[\textbf{RQ2.1:}] With how many training examples does a
	technique perform best?
  \item[\textbf{RQ2.2:}] How structurally variable can training
  examples be?
  \item[\textbf{RQ2.3:}] How accurate are the retrievals of a technique?
\end{itemize}
\end{simplebox}

In the following, we first introduce the data set we created for this
study, and then describe the study design and its
results.
We answer these RQs separately for each technique.

\subsection{LogChunks Data Set}
\label{sec:logchunks}
To conduct the study in this paper, we created the
\emph{LogChunks} data set~\citep{brandt2020logchunks}.
We collected 797 build logs that
from 80 GitHub projects and 29
programming languages on Travis CI, the most-used source of build logs
according to our literature mapping.
This wide variety of projects and the large number of different build
tools they employ enables us to measure the generic
applicability of chunk retrieval techniques.

For each build log in \emph{LogChunks}, we manually marked
the chunk that describes why the build
failed---the information targeted by most of the works in our
literature mapping (see \Cref{sec:lit-sur:discussion}).
To be able to configure the keyword search technique,
we associated each chunk in the data set with keywords that we
would use to search for the selected chunk within the log.
In
addition, we categorized the log chunks according to their format
within the log.
If the chunks were, for example, surrounded by the same markings
within the log, we assigned them the \emph{same structural category},
as described in \Cref{sec:crt-example}.

As explained in \Cref{sec:crt-characteristics}, examples are the basic
unit by which we configure each technique.
The full build log text,
the chunk, the search keywords, and the assigned
structural category make
up one \emph{example} in \emph{LogChunks}.

\subsection{Study Procedure}

In the following, we explain our study procedure
and introduce the metrics that
we measure to answer RQ2.
\Cref{fig:study} depicts the individual steps of our study as an
algorithmic procedure.

For each chunk retrieval technique and each project, we create
training example sets consisting of 1, 2, \dots 5 examples (lines
\circlenum{1}--\circlenum{3}).
We vary the training set size to measure how many examples we need to
confidently configure a technique, addressing \textbf{RQ2.1}.
While this is atypical for an IR-based technique such as CTS---where
the default is to split the whole corpus at a given percentage, often
80/20---we did this for the specific task of build logs.
We want to find out how many examples one has to manually specify with a
lot of effort~\Cref{sec:lit-sur:discussion} and at which point techniques
become efficient.

The test set consists of the first build log produced
chronologically after the build logs in the training set \circlenum{5}.
This way, we train on examples from past build logs and test on
a more recent log, just as one would apply these techniques in the
real world.

In line \circlenum{6}, we obtain how many different structural categories
are present in the trained logs and store this to be able to answer
\textbf{RQ2.2}.

In the next step, we configure the chosen chunk retrieval technique with
the examples from the training set \circlenum{8} and run the technique
on the
test set, producing the retrieval output \circlenum{9}.

Based on this, we calculate the following metrics in
\emph{calculateMetrics}, addressing \textbf{RQ2.3}:

\vspace{0.2cm}
\begin{itemize}[leftmargin=0.4cm] \itemsep1em
	\item $|\mbox{True\ Positives}| = \mathit{desiredLines} \cap
	\mathit{retrievedLines}$ \vspace{0.2cm}\\
	True positives are lines that appear both in the output of a
	technique as well as the chunk defined in the
  test example.

	% What about when a line is replicated twice?
	% I.e., Are line numbers part of this? => no.
	% not checked, but if lines are identical they also contain
	% the same information so if someone asks we can defend I think

	\item $\mbox{Precision} = \dfrac{|\mathit{True\
	Positives}|}{|\mathit{retrievedLines}|}$ \vspace{0.21cm} \\
	Precision of a chunk retrieval describes which proportion of
	the retrieved lines were actually desired.

	\item $\mbox{Recall} =
	\dfrac{|\mathit{True\ Positives}|}{|\mathit{desiredLines}|}$
	\vspace{0.2cm} \\
	Recall of a chunk retrieval describes which proportion of the
	desired lines were retrieved.

	\item $\mbox{F$_{1}$-score} = 2 \cdot \dfrac{\mathit{Precision}
	\cdot \mathit{Recall}}{\mathit{Precision} + \mathit{Recall}}$
	\vspace{0.2cm}\\
	In addition, we calculate the F$_{1}$-score, the harmonic mean
	of precision and recall.
  We prefer F$_{1}$ to other aggregate
	measures such as accuracy because of the
	``needle-in-the-haystack'' scenario, we want to avoid bloating
	our results by correctly not finding lots of irrelevant log
	lines.

	% actually that is now giving recall another (nicer) name
	% improved if we just talk about recall later?
	\item Successful retrieval = $\mathit{true}\ \mathit{iff}\
	\mathit{Recall} = 1, \vspace{0.1cm}\\$
	Partially successful retrieval = $\mathit{true}\ \mathit{iff}\
	0<\mathit{Recall}<1, \vspace{0.1cm}\\$
	Unsuccessful retrieval = $\mathit{true}\ \mathit{iff}\
	\mathit{Recall} = 0, \vspace{0.1cm}$

	We define a successful retrieval as one where all desired
	lines were extracted, therefore when recall is one.
	A retrieval is partially successful if at least some of the
	desired lines were extracted,
	and if the retrieved lines contain none of the desired lines
	a retrieval was unsuccessful.
\end{itemize}
\vspace{0.2cm}

\noindent
Finally, we aggregate the results to be able to reason per technique
\circlenum{15}, completing our study design to answer \textbf{RQ2}
overall.

\subsection{Results}
In this section, we first present the metrics for each chunk retrieval
technique separately.
Afterwards, we compare the three techniques with each other and with a
random baseline.

\subsubsection{Programming by Example (PBE)}
\label{sec:r:pbe}

\begin{figure}[tbp]
		\centering
		\includegraphics[width=0.8\columnwidth,
		clip]{img/big-study/failure-reason-pbe.pdf}
		\caption{High-level results of chunk retrieval with
		Programming by Example (PBE).}
		\label{fig:failure-reason-PBE}
\end{figure}

\lstset{
  language=,
  morekeywords={Test, Output, Desired},
  keywordstyle=\textbf,
	frame=single
}
\begin{figure}[tbp]
  \centering
\begin{subfigure}[b]{\columnwidth}
  \begin{lstlisting}[breaklines=true,frame=tlr]
[0K$ test/sass-compile-tester.sh
  \end{lstlisting}
  \vspace{-\baselineskip}
  \lstinputlisting[backgroundcolor=\color{Yellow!60},breaklines=true,frame=rl]
	{listings/chunk3.txt}
  \vspace{-\baselineskip}
  \begin{lstlisting}[breaklines=true,frame=blr]
	from line 5 of sass/components/_all.sass
  \end{lstlisting}
	\caption{Actual chunk retrieval output ($retrievedLines$
	in yellow).}
	\label{lst:pbe-part-success-output}
\end{subfigure}

\begin{subfigure}[b]{\columnwidth}
  \begin{lstlisting}[breaklines=true,frame=tlr]
[0K$ test/sass-compile-tester.sh
  \end{lstlisting}
  \vspace{-\baselineskip}
  \lstinputlisting[backgroundcolor=\color{Cerulean!60},breaklines=true,frame=rl]
	{listings/chunk3.txt}
  \vspace{-\baselineskip}
  \begin{lstlisting}[backgroundcolor=\color{Cerulean!60},breaklines=true,frame=rl]
	from line 5 of sass/components/_all.sass
	from line 6 of bulma.sass
  \end{lstlisting}
  \vspace{-\baselineskip}
  \begin{lstlisting}[breaklines=true,frame=blr]
  Use --trace for backtrace.
  \end{lstlisting}
	\caption{Targeted chunk ($desiredLines$ in blue).}
	\label{lst:pbe-part-success-desired}
\end{subfigure}
  \caption{Example of a partially successful retrieval
  (PBE retrieved only
  two of the four targeted lines).}
  \label{lst:pbe-partially-successful}
\end{figure}

\begin{figure*}
\centering
    \textbf{Programming by Example (PBE)}\par\medskip
\begin{subfigure}[b]{\columnwidth}
		\centering
		\includegraphics[width=0.8\columnwidth,
		clip]{img/big-study/failure-reason-categorycount-PBE.pdf}
				\caption{Successfulness of retrieval
				compared by structural category count
				in training and test sets.}
		\label{fig:failure-reason-categorycount-PBE}
\end{subfigure}\hspace{\fill}
\begin{subfigure}[b]{\columnwidth}
		\centering
		\includegraphics[width=0.8\columnwidth,
		clip]{img/big-study/recall-precision-examplecount-sythesisworked-PBE.pdf}
				\caption{Precision, recall, and
				F$_{1}$-score when PBE could synthesize
				a consistent program compared with the
				size of the training set.}
		\label{fig:recall-precision-examplecount-sythesisworked-PBE}
\end{subfigure}
\caption{Results of chunk retrieval with Programming by Example
(PBE).}
\end{figure*}

% actually, we could cut this figure and move it's explanation
% to the next one (this is an aggregation of the next one)
% however, having them separate eases the entry into the results a bit
% and we have explanation of the categories before the more complicated
% aspect of the change with more structural categories
\Cref{fig:failure-reason-PBE} shows the results of chunk
retrieval with PBE.
It shows how many of the retrieval runs were successful,
partially successful, or unsuccessful,
as well as the number of runs where the
synthesized program did not match on the analyzed build log or where
no program could be synthesized.
% The last two results stem from the regular expressions and their
% synthesis, therefore only apply to PBE.
Out of the 400 runs, 5 per each one of the 80 projects,
PBE extracted all desired lines successfully in 138 cases.
In 18 cases, the synthesized program only extracted a
subset of the desired lines, while in 12 cases it extracted none of
the desired lines.
For the 18 partially successful runs, the average recall
was 47\%.
\Cref{lst:pbe-partially-successful} shows an example of such
a partially successful retrieval.
In it, the synthesized program only
retrieved two (\Cref{lst:pbe-part-success-output})
of the four (\Cref{lst:pbe-part-success-desired}) targeted lines.
In 59 cases, PBE could synthesize a regular expression program, though
the program did not match on the test build log.
In 173 of the 400 cases
could PROSE not synthesize a program consistent with all of the training
examples.

\Cref{fig:failure-reason-categorycount-PBE} shows how
the results of PBE
runs depend on the number of structural categories in the training and
test examples.
% It shows the categories from above change depending on how many
% structural categories are present.
The figure demonstrates that programming by example mostly
returns exactly the desired output when there is only one
structural category present in the training and test examples.
However, when
two or more structural categories are simultaneously present,
PROSE could in most cases not synthesize a program.
For four or more present
categories PROSE could never synthesize a consistent program.

Zooming in on only the 227 runs where PBE could synthesize a program,
\Cref{fig:recall-precision-examplecount-sythesisworked-PBE}
presents violin plots and averages of the precision, the recall
and the F$_{1}$-score
of these runs compared with the number of examples in the training set.
We chose violin plots to show spread in distributions of values.
Specifically, we see an hour glass shape for training set size 1.
In this case, it means that for recall (and F$_1$-score) there are about
as many observations with low recall as there are with high.
This is a striking difference to set size 2, where we have mostly
correct extractions.
When the training set
size increases from one to two, recall, and F$_{1}$-score increase by
about 25\%, precision increases by about 10\%.
For two or more
training examples, recall, and F$_{1}$-score stay around 75\% and
precision around 96\%.

\subsubsection{Common Text Similarity (CTS)}
\label{sec:r:cts}

\begin{figure*}
\centering
    \textbf{Common Text Similarity (CTS)}\par\medskip
\begin{subfigure}[tb]{\columnwidth}
		\centering
		\includegraphics[width=0.8\columnwidth,
		clip]{img/big-study/recall-precision-examplecount-CTS.pdf}
		\caption{training set size.}
		\label{fig:recall-precision-examplecount-CTS}

\end{subfigure}\hspace{\fill}
\begin{subfigure}[tb]{\columnwidth}
		\centering
				\includegraphics[width=0.8\columnwidth,
				clip]{img/big-study/recall-precision-categorycount-CTS.pdf}
		\caption{structural category count
		in training and test set.}
		\label{fig:recall-precision-categorycount-CTS}
\end{subfigure}
\caption{Precision, recall, and F$_{1}$-score of chunk
retrieval with Common Text Similarity (CTS) compared by \ldots}
\label{fig:results-CTS}
\end{figure*}

% The following sections are each quite short...
% we could merge them, but then it loses consistency with the other
% result subsections where each plot gets an own paragraph
\Cref{fig:results-CTS} shows violin plots
of the precision,
recall, and F$_{1}$-score of the chunk retrieval runs with CTS.
The black horizontal lines show the average of these measurements.
For some measurements, the violin plot is not visible because there is
only one available data point or all data points have the same value.

\Cref{fig:recall-precision-examplecount-CTS} presents the
measurements for an
increasing number of training examples.
When using one to five
training examples, the size of the training set has no noticeable
influence on precision, recall or F$_{1}$-score of the chunk retrieval
with CTS.

\Cref{fig:recall-precision-categorycount-CTS} shows the
measurements for an increasing number of structural categories in the
training and test examples.
With increasing category count, precision,
recall, and F$_{1}$-score decrease.
% Especially for more than four
% categories present we have no chunk retrieval runs where all desired
% lines were extracted.

\subsubsection{Keyword Search (KWS)}
\label{sec:r:kws}
\begin{figure*}
\centering
    \textbf{Keyword Search (KWS)}\par\medskip
\begin{subfigure}[b]{\columnwidth}
		\centering
		\includegraphics[width=0.8\columnwidth,
		clip]{img/big-study/recall-precision-examplecount-KWS.pdf}
		\caption{training set size.}
		\label{fig:recall-precision-examplecount-KWS}
\end{subfigure}\hspace{\fill}
\begin{subfigure}[b]{\columnwidth}
		\centering
		\includegraphics[width=0.8\columnwidth,
		clip]{img/big-study/recall-precision-categorycount-KWS.pdf}
		\caption{structural category count
		in training and test sets.}
		\label{fig:recall-precision-categorycount-KWS}
\end{subfigure}
\caption{Precision, recall, and F$_{1}$-score of chunk retrieval with
Keyword Search (KWS) compared by \ldots}
\label{fig:results-KWS}
\end{figure*}


We present the precision,
recall, and F$_{1}$-score of the chunk retrieval runs with KWS in
\Cref{fig:results-KWS}.
Violin plots show the distribution the values of each measurement,
while the black horizontal lines show their average.
If the figure shows no violin plot for a measurement, we only have one
data point available or all values are the same.

\Cref{fig:recall-precision-examplecount-KWS} presents these
measurements for different
numbers of training examples.
The recall increases by about 12\% when
increasing the size of the training set to more than one example,
while the precision stays constant around 16\%.
The F$_{1}$-score
stays around 26\%.

\Cref{fig:recall-precision-categorycount-KWS} shows the same
measurements for an increasing number of structural categories in the
training and test examples.
For more than one structural category in
the training and test examples the recall decreases by about 20\% and
the precision decreases about 6\%.
For more than two structural
categories there is no clear trend in precision, recall, or
F$_{1}$-score.

% M: Perhaps, above in our section about logchunks, we would need some
% basic empirical stats -- e.g., how long are the files and how much
% do we usually extract from the log
% C: yup, that would be nice
% sadly, it's quite a bit of work to get those :/
% maybe I/We'll do it at the end
\subsubsection{Random Line Retrieval (RLR)}
\label{sec:r:rlr}
The baseline of randomly
picking lines from build logs delivers the expected low and
``random'' results: precision ranges between 5\% and 8\%,
recall between 6\% and 8\%.
As expected, the size of the training set has no impact on precision
or recall of retrieving chunks with RLR.
With a larger number of structural categories within the training and
test sets, the precision of RLR decreases from 7\% to 0\%, while
the recall varies between 0\% and 11\%.

\subsubsection{Comparison of All Techniques}
In this section, we compare the results of all four chunk retrieval
techniques and show the impact of structural categories on precision,
recall and F$_{1}$-scores of the different techniques.

% absolute results? is there a nice word for these results
\Cref{fig:success-partial-all} compares the results all of chunk
retrievals techniques in our study.
It shows how many runs per technique were successful, partially
successful or unsuccessful in retrieving the chunk defined in the
test example.
CTS and KWS
extract at least some of the desired lines in 79\% and 88.5\%
of the chunk retrieval runs.
With 38.25\%, KWS also has the highest proportion of fully
successful extractions, followed by PBE with 34.5\%.
PBE has the
lowest number of partial retrievals with only 18 out of 400 chunk
retrieval runs.
All techniques clearly outperform the random baseline (RLR).

\Cref{fig:category-all}
shows the influence of all training and test examples being from the
same structural category
(\Cref{fig:recall-precision-singlecategory-all})
compared to being from multiple categories
(\Cref{fig:recall-precision-multicategory-all}).
For more
than one category present, the recall of PBE decreases greatly.
For CTS and KWS the values also decrease, while RLR is not affected by
the number of structural categories present.
Again, all techniques---except for PBE with multiple structural
categories---yield better retrievals than RLR.

The runtime of the evaluation varied from technique to technique.
The 400 runs of PBE took 19 hours to complete on a dedicated server.
This is due to the long time needed for the program synthesis to
complete, which increases the more training examples are used and
ranges from about 10 to 30 minutes.
Running a configured technique takes a similar amount of time for all
techniques.
Executing the 400 evaluation runs for CTS took 15 minutes, for KWS 25
minutes and for RLR 5 minutes.

\begin{figure}[!t]
		\centering
		\includegraphics[width=0.8\columnwidth,
		clip]{img/big-study/success-partial-all.pdf}
		\caption{Success of chunk retrievals across all
		techniques.}
		\label{fig:success-partial-all}
\end{figure}

\begin{figure*}
\centering
    \textbf{PBE, CTS, KWS, and RLR}\par\medskip
\begin{subfigure}[b]{\columnwidth}
		\centering
				\includegraphics[width=0.8\columnwidth,
				clip]{img/big-study/recall-precision-singlecategory-all.pdf}
		\caption{Training and test examples in 1
		structural category.}
		\label{fig:recall-precision-singlecategory-all}
\end{subfigure}\hspace{\fill}
\begin{subfigure}[b]{\columnwidth}
		\centering
				\centering
		\includegraphics[width=0.8\columnwidth,
		clip]{img/big-study/recall-precision-multicategory-all.pdf}
		\caption{Training and test examples in \textgreater
		1 structural
		category.}
		\label{fig:recall-precision-multicategory-all}
\end{subfigure}
\caption{Precision, recall, and F$_{1}$-score of all
techniques compared, split by structural category count.}
\label{fig:category-all}
\end{figure*}

\subsection{Discussion}
\label{sec:discussion}

Following from the results of the empirical study, in this section,
we give recommendations for
which types of log chunks and how many training examples
each technique is suited, as well as
how their output can be used further.
We summarize these recommendations in
\Cref{tab:single-technique-recommendations}.

\begin{table}[tbp]
\caption{Recommendations for each of the investigated chunk retrieval
techniques.}
\label{tab:single-technique-recommendations}

\resizebox{\columnwidth}{!}{%
\centering
\begin{tabular}{lllll}
  \toprule
  & & \textbf{PBE} & \textbf{CTS} & \textbf{KWS} \\

	\midrule

	\textbf{RQ2.1} & \textbf{Training Set Size} & 2 & no influence &
	2 \\

  \midrule

	\textbf{RQ2.2} & \textbf{Structural Categories} & 1 & less
	is better
	& \makecell[l]{best 1 \\ multiple okay} \\

	\midrule

  \multirow{6}{*}{\textbf{RQ2.3}} & \textbf{Precision} &
	\makecell[l]{high \\ (if synthesis succeeds)} &
  medium & low \\

	\cmidrule{2-5}

  & \textbf{Recall} & \makecell[l]{high \\ (if synthesis succeeds)} &
  medium & high \\

	\cmidrule{2-5}

  & \makecell[l]{\textbf{Confidence in} \\ \textbf{Output Correctness}}
  & high & low &
  \makecell[l]{low (precision) \\ high (recall)} \\

	\cmidrule{2-5}

  & \textbf{Output Consumer} & program & human & human \\
  \bottomrule
\end{tabular}%
}
\end{table}

\subsubsection{Programming by Example (PBE)}

\noindent
\textbf{With how many training examples does PBE perform best (RQ2.1)?}
When the training examples are of the same structure, one or two
two training examples are enough input for PROSE to synthesize a regular
expression program with good recall.
In the study, additional training
examples from the same structural category
did not improve the chunk retrieval.
In fact, unless they
were in some sense redundant, adding more training examples even
hindered the applicability of PBE.

\noindent
\textbf{How structurally variable can training examples be (RQ2.2)?}
The study results show that chunk retrieval with PBE gives best
results when the log chunks are structurally identical.
PBE is thus suited to retrieve information
chunks that always have the same defining surrounding or internal
structure.
To extract, e.g., the reason a build failed, the log
passage describing the failure would always have to start and
end the same way.

\noindent
\textbf{How accurate are the retrievals of PBE (RQ2.3)?}
If the program synthesis succeeds and running the regular expression
program yields an output, PBE has high precision and recall.
The technique
clearly identifies a failing synthesis or when the regular expression
finds no match on a build log.
Therefore, if PBE produces
an output, the user can have high confidence that it is the desired
output.
This preciseness makes output from PBE chunk retrieval
well-suited for machine consumption and therefore automatic onward
processing.

\subsubsection{Common Text Similarity (CTS)}

\noindent
\textbf{With how many training examples does CTS perform best (RQ2.1)?}
The number of training examples had no noticeable influence on
precision or recall.
Information retrieval techniques
like text similarity commonly learn on a higher number of examples
than we chose in the study.
Future work should investigate how many
examples yield improvements in the chunk retrieval over a single
training example.

\noindent
\textbf{How structurally variable can training examples be (RQ2.2)?}
CTS yields better results the
fewer structural categories are present in the training and test
examples.

\noindent
\textbf{How accurate are the retrievals of CTS (RQ2.3)?}
CTS has good precision and recall on average, though with a high
variation.
This means that the quality of an output by CTS is hard to
determine, which makes it unsuited for automatic processing and requires
a human to further inspect and interpret the output.
This could include semi-automated procedures such as sending
developers an email with the extracted build failure reason.

\subsubsection{Keyword Search (KWS)}
\noindent
\textbf{With how many training examples does KWS perform best (RQ2.1)?}
Going from one to two training examples, KWS's recall improves
significantly.
However, further enlarging the training set
does not lead to further improvements.

\noindent
\textbf{How structurally variable can training examples be (RQ2.2)?}
KWS has a higher recall than the two other techniques for multiple
structural categories present in the training and test examples.
This
makes KWS a good technique if there is little prior knowledge of how
the targeted log chunk is represented in the build log.
For the
example of extracting the reason the build failed, KWS is best suited
if a build can fail in various steps logged by different build
tools and no
pre-categorization of where the build failed is available.

\noindent
\textbf{How accurate are the retrievals of KWS (RQ2.3)?}
While KWS has the highest recall of all three techniques, its
precision is the lowest.
The output of a chunk retrieval with KWS is
well-suited to be read by humans, but ill-suited for consumption by
automated tools.


\subsection{Usage Recommendation}
In this section, we derive a decision tree to help practitioners and
researchers choose a suitable chunk retrieval technique.
We then
illustrate its use on two examples.

\subsubsection{Derivation of Decision Tree}
\begin{figure}[bt]
		\centering
		\includegraphics[width=0.8\columnwidth,
		clip]{img/crt-recommendation.pdf}
		\caption{Decision tree for chunk
		retrieval techniques.}
		\label{fig:crt-recommendation}
\end{figure}

Abstractly, we have learned that every chunk retrieval technique
differs in its suitability depending on the kinds of logs it should
operate on and the sort of output one expects.
In this section, we
unify these findings, culminating in the creation of a decision tree
to help developers and researchers choose an adequate technique in
\Cref{fig:crt-recommendation}.

The first and most important aspect that influences the decision for a
chunk retrieval technique is the presence of different structural
categories in the log (or lack therefore).
Are the targeted log
chunks always present in the same structural way
within the build logs? If so, the chunks
in all training examples and the analyzed build log are in the same
structural category (first question in \Cref{fig:crt-recommendation}).

On the leftmost path of the decision tree,
when the chunks are from one structural category and the user
needs high confidence in the correctness of the output or prefers
no output over output with low recall, we recommend PBE\@.
If the user does not
require high confidence in the correctness of the output and prefers
to receive some output even with low recall, we recommend CTS\@.

On the rightmost path of the decision tree,
if the chunks are from multiple structural categories and recall is
more important than precision, we recommend to use KWS\@.
If precision is more important than recall, we
recommend CTS\@.

As a final recommendation, one could create a ``super analyzer'' by
combining the different build log analysis techniques studied in this
paper.
Such a super analyzer would likely always first employ PBE
(because of its high accuracy), followed by a combination of CTS and
KWS\@.
Other than initial setup and training time, there is no
downside to this approach if it is implemented in a transparent way:
the output of the super analyzer could include from which
sub-technique it originated and thus facilitate automatic onward
processing or interpretation of the result.


\subsubsection{Example Use Cases}
To illustrate how one would use the decision tree to find a suitable
chunk retrieval technique we describe here two concrete examples:
a software team monitoring build performance and a
group of researchers investigating why builds fail.

In the first example, a software development team wants to monitor
the performance of the phases within their CI build.
They are using
Travis CI, which documents the duration of build phases
within the build log.
As all relevant log statements are formatted the same way,
the targeted log chunks are from one structural category.
Therefore, following the leftmost path in
\Cref{fig:crt-recommendation},
they should use PBE.

In the second example, researchers wants to investigate the names
of test cases that cause build failures.
Their study should cover a wide range of test tools, therefore are the
log chunks they target in various, non-predictable structural
representations.
As they have to manually inspect the results of both CTS
and KWS, they prefer higher recall over higher precision to
avoid inspecting
the whole log in case the relevant chunk was not retrieved.
Therefore, following the rightmost path in
\Cref{fig:crt-recommendation},
the decision tree recommends the researcher to use KWS\@.

An alternate, more precise solution
could be for researchers to first separate the build logs by the
test tool responsible for logging the results.
Then, the
targeted log chunks are from one structural category and they can use
PBE, trained with examples from each test tool separately.


\section{Threats to Validity}
In this section, we outline threats to the validity of our work and
show how we mitigated them.

% M: fill in the maven analyzer plugin and others.
% C: added to the jenkins bl analyzer, though I left out th maven
% analyzer cause it has 1 star on github so kinda not 'industry' grade
\textbf{Limitation to Scholarly Articles.} We limited our literature
mapping to scholarly articles.
However, the field of build log analysis is extremely relevant to
practitioners, too.
While it is difficult to systematically sight all such non-scientific
material, we ran several Google search queries related to build log
analysis and evaluated them informally.
Our findings include the Jenkins build log
analyzer\footnote{https://plugins.jenkins.io/build-failure-analyzer/}, where a user can
define custom regular expressions to parse a build log, and a web page
on which Microsoft details how its CI service, Azure Pipelines, uses a
PBE approach for certain log formats such as Python.
This inspired the choice of PBE in \Cref{sec:choice}.
Our impression is that non-scientific tools are often based on
parsing with the help of regular expressions---similar to the
techniques mentioned in scientific literature and the PBE technique
we proposed.

\textbf{Implementation.}
Our results depend on the implementation of the investigated chunk
retrieval techniques and the libraries we used.

The program synthesis
provided by PROSE is the basis for our implementation of PBE\@.
The
idiosyncrasies of this framework influence the PBE results.
Other
frameworks similar to PROSE might have somewhat different strengths
and weaknesses.
For example, the need for examples from a single
structural category stems from the fact that PROSE cannot learn
regular expression programs with arbitrary boolean conjunctions and
disjunctions~\citep{mayer2015user}.
PROSE introduced this constraint
to keep the synthesis performance reasonable.
At the same time, this
is clearly a current theoretical challenge of all PBE implementations,
and we therefore attribute it to the technique itself, rather than the
specific implementation.

Similarly, the library
{\tt text2vec}\footnote{http://text2vec.org/}
and the way it splits strings into word tokens is influencing
our implementation of CTS\@.
On the other hand,
{\tt text2vec} is the de-facto standard library to do word embeddings
in R.
We intentionally chose a simple, minimally configured or tuned
approach to compare against.
Tuning the text similarity
meta-parameters more to the specific use case of chunk retrieval from
build logs would yield better chunk retrieval results.

Overall, we believe our implementations and their weaknesses stand
prototypical for the techniques.
We tried to limit idiosyncrasies as
much as possible, by choosing standard approaches and minimizing
configuration, instead of parameterizing and fine-tuning methods
specific to our data set.

\textbf{Data Set.}
Just like the implementations, the specific build logs from the
\emph{LogChunks} data set affect
the outcomes of the comparison study.
\emph{LogChunks} consists of build
logs from open source projects and therefore it is unclear to which
degree our results apply to industry projects that use build tools not
popular in open source development.
However, since \emph{LogChunks}
covers a wide array of build tools and programming languages, we
believe that the findings do generalize.
In addition, build log output
of such a tool would need to be wildly different from how the over 27
tools covered in our study do it---this seems unlikely.

We collected build logs from
Travis CI.
Yet, the format of the log chunk we chose for the
evaluation is largely independent of Travis CI\@, because the reason
the build failed is described within the build logs by the tools
themselves and not the Travis CI environment.

\textbf{Training Set Size.}
Especially the results for CTS might be influenced by the fact that we
only trained on one to five examples.
We chose this small training
set size as a human has to provide the training examples and one of
the central aims of the chunk retrieval techniques is that they can be
set up wit little effort.
The fact that KWS and CTS performed best
with already two training examples indicates that the maximum training
set size of five is sufficiently large for research evaluation.

\textbf{Few Samples with Many Structural Categories.}
Our comparison study includes fewer measurements with many structural
categories than with one structural category category.
50\% of the measurements were conducted with one structural category
in the training and test sets
(34\% with two, 16\% with more than two categories).
This distribution mirrors the
structural categories in a real-world data set, where there were
mostly few structural categories within one project.
In addition, the
third of the measurements we conducted with two structural categories
already showed the negative effect of an increasing number of
structural categories on the performance of the chunk retrieval
techniques.

\section{Future Work}
In this section, we give an outline of fruitful future
research directions in the field of build log analysis.
We base our
suggestions on the results of the literature mapping and the
findings from our technique comparison study.

\textbf{Systematic Mapping of Industry Approaches for Build
Log Analysis.}	Our mapping is limited to scholarly material.
However,
handling build logs is a challenge for a wide range of practitioners
as well.
We propose to investigate the techniques used in industry to
analyze build logs.
An example is the
Jenkins \emph{build-failure-analyzer}
plugin\footnote{https://plugins.jenkins.io/build-failure-analyzer/} or automatically inferred
test results in Azure\footnote{https://docs.microsoft.com/en-us/azure/devops/pipelines/test/review-continuous-test-results-after-build?view=azure-devops\#automatically\_inferred\_tests}.
Such a study might
also need to tackle the more abstract problem of how to map
non-scholarly material systematically, which would be an important
contribution to Software Engineering research.

\textbf{Further Analysis of \emph{LogChunks}.}
We created the
\emph{LogChunks} data set \citep{brandt2020logchunks} specifically for
the comparative
study in this paper, though it can be the basis for various further
analyses of build log data.
For example, we can investigate the keywords to answer which
keywords developers use to mark errors that lead to a build failure
within build logs.

\textbf{Cross-Project Build Log Analysis.}
We trained and
tested each chunk retrieval technique on examples from the same
project.
We propose to analyze how techniques could be trained
across projects, building the cornerstones for build
environment-agnostic analysis tools.
\todo{for tools that do not have to be manually configure to fit a
specific project or build environment}
This has the advantage of creating
a default build log analyzer that would work without any pre-training.

\textbf{Comparison with more Chunk Retrieval Techniques.}
In this article, we investigated the three chunk retrieval techniques PBE,
CTS, and
KWS\@.
Our study design can be reused to evaluate other build log
analysis techniques, such as for example deep learning-based
techniques (discussed in \Cref{sec:choice}) or manually-written
programs such as the parser created for
BugSwarm~\citep{tomassi2019bugswarm} or the regex-based approach
created for TravisTorrent~\citep{beller2017oops}.

One could also interpret the techniques we implemented
differently.
For example, for CTS, we look for similarity between the
lines we want to extract.
One could find similarity between the
context that surrounds them, instead of the actual chunk to be
extracted.
The intuition behind this approach would be that often the
error chunk is variable, but the context around it is
stable---essentially the principle by which extraction by PBE
works.
Such a version of ``inverse CTS'' might yield better retrieval
results.

\textbf{Refinement of Retrieval Quality.} We
investigated basic configurations of existing techniques applied to
chunk retrieval from build logs.
In a next step, each of these
techniques could be refined to better customize it to the domain of build
logs.
The \emph{LogChunks} data set and our study results can act as a
baseline to benchmark such technique improvements.
We propose the
following improvements:
\begin{itemize}[leftmargin=0.4cm]
  \item \textbf{Custom Ranking and Tokens for PBE.} Program
  synthesis through PROSE ranks possible programs according to
  what the user most likely intended.
  One could adapt the ranking
  rules provided by the \emph{FlashExtract} DSL to fit common build log
  chunk retrieval tasks.
  \emph{FlashExtract} includes special tokens when
  enumerating possible regular expressions~\citep{le2014flashextract:}.
  One could extend these
  with tokens found in build logs, such as ``-,''
	``=,'' ``ERROR,''
  or ``[OK]'' to improve results.
  \item \textbf{Meta-Parameter Optimization for CTS.} Information
  retrieval techniques have various meta-parameters which can be
  optimized for the specific use
  case~\citep{panichella2016parameterizing}.
  We propose to further
  investigate improvements in preprocessing of the log text, in
  tokenization of the log lines into terms, and in stop words
  lists.

  For CTS, we currently have no concept of semantic
  similarity.
	Therefore, the synonyms ``Fail'' and ``Failure'' would
  fall into two different frequency counts.
	Using a
  wordnet~\citep{pedersen2004wordnet} or other NLP techniques to
  capture synonyms might improve similarity measure between related
  lines, and therefore overall performance of CTS.
\end{itemize}

\textbf{Usability Analysis of Chunk Retrieval Output.}
Our
analysis of the chunk retrieval output focuses on
precision and recall.
A next step could be to investigate how useful these
outputs really are to developers through controlled experiments or
user studies.

\section{Conclusion}
\label{sec:conclusion-fw}

The goal of this paper is to support practitioners and researchers in
their decision on how to analyze build logs.
To this end, we
conducted a systematic mapping to investigate how researchers
currently analyze build logs.
We saw that build log analysis is an emerging field.
At the same time, articles largely do not justify why
they use a certain technique, nor do they document it in detail or
open-source its implementation.
Moreover, the mapping showed that,
while there is a niche for custom parsers and plugins for very popular
tools, such as the Java ecosystem, there is a wide variety of build
tool and environment combinations for which the creation of such
bespoke tooling is unlikely.
Overall, this study showed the need for a new
\todo{not generic, but configurable}generation of generic and automated build log analysis techniques that
need little effort to set up, both for practitioners as well as
researchers.

Based on common approaches we observed in literature,
we implemented three promising chunk retrieval techniques:
PBE synthesizes regular expression programs based on examples of logs
and corresponding chunks, CTS leverages a common text similarity
measure to retrieve those lines which are most similar to example
chunks, and KWS finds the targeted chunks in a build log by searching
for identifying keywords.

We evaluated the performance of these chunk retrieval techniques on the
\emph{LogChunks} data set and saw varying results.
CTS had the highest average F$_{1}$-score with 51\%, while the other
techniques showed higher average scores for precision (95\%, PBE) and
recall (70\%, KWS).
Overall, all techniques vastly outperformed a
baseline of random line retrieval.

To guide researchers and developers on which technique suits their
use case,
we summarized our results in a decision tree.
We found
that the structural representation of the targeted information in the
build logs is the main factor one should consider when
choosing a build log analysis
technique.
Secondary factors are the desired confidence in recall and
precision of the produced output and whether precision or recall is
more important for the task at hand.

Finally, we gave an
outline to guide future research efforts in the area of build log
analysis.
Important themes are a systematic investigation of industry approaches,
benchmarks of existing build log analysis approaches, and further
refinement of the chunk retrieval techniques we proposed.
We strongly believe that further research into the techniques to
analyze build logs will support researchers and developers alike to
leverage the vast amounts of valuable information hidden
in build logs.

\begin{acknowledgements}
We would like to thank Annibale Panichella for his valuable input during the conception, implementation and evaluation of the chunk retrieval techniques.
\end{acknowledgements}


% Authors must disclose all relationships or interests that 
% could have direct or potential influence or impart bias on 
% the work: 
%
% \section*{Conflict of interest}
%
% The authors declare that they have no conflict of interest.


% BibTeX users please use one of
\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
%\bibliography{}   % name your BibTeX data base

\bibliography{paper}

\end{document}
% end of file template.tex

