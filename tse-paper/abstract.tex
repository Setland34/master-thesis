Build logs are the textual by-product that a software build process
creates, often as part of its Continuous Integration (CI)
pipeline. Build logs can not only serve as a paramount source of
information for developers to understand and debugging into build or
test failures, but also for automated on-ward processing. Manually
extracting the important chunks of information, though, is akin to
finding a needle in a haystack. Recently, attempts to partly automate
this time-consuming activity have come up, such as rule- or
information-retrieval-based techniques.
In this paper, we first sysmetically overview and categorize the existing methods to extract information chunks. We then develop prototypical implementations for three promising techniques, namely program synthesis
by example, textual similarity and search keywords. We evaluate them in an empirical study on the manually
labeled \emph{LogChunks} data set, which comprises more than x build logs in y languages.
Our findings show that none of the three techniques in general outperforms
the others. We discuss under which circumstances each technique performs best
and provide a recommendation on when developers or researchers should use which
technique.

% Software builds from continuous integration produce detailed logs about the status and results of the various tools involved in the build.
% These build logs are a valuable data source for developers and researchers to inspect test results, the duration of build steps or understand the cause of a build failure.
% However, build logs are very verbose, at best semi-structured and their structure differs highly between projects.
% This makes it hard to process and analyze them.
% In this paper, we evaluate and compare three different techniques that aim to retrieve specified text chunks from a build log, namely program synthesis by example, textual similarity and search keywords.
% We conduct an empirical study by comparing these techniques on the \emph{LogChunks} data set of 797 Travis CI logs from a diverse range of projects.
% Our findings show that none of the three techniques clearly outperforms the others.
% We discuss under which circumstances each technique performs best and provide a recommendation on when developers or researchers should use which technique.
