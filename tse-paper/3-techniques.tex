
\section{Chunk Retrieval Techniques}
\label{sec:techniques}
This section introduces key concepts of the three chunk retrieval
techniques we study in this article, namely program synthesis by
example (PBE), common text similarity (CTS), and keyword search (KWS).
We use these techniques to extract text chunks from build logs that
represent a specific, targeted information.

\subsection{Characteristics of Chunk Retrieval Techniques}
\label{sec:blirt}
In this article, we evaluate different techniques to automatically
retrieve pieces of information that appear literally in the build
logs, i.e., techniques that do not aggreagte, combine, or deducted
information. We call such pieces of information from build logs
\emph{chunks}, and the techniques \emph{chunk retrieval techniques}.
The techniques we investigate here do not require a formal lexer and
parser to analyze the entire structure of build logs, but focus on
ad-hoc extracting just one specific piece of information per
configuration.

With the term \textit{configuration,} we abstract over the training
and parametrization that different techniques require in different
forms. A configuration can be explicitly stated or implicitly dervied
by learning through provided examples. It is therefore a manual
specification of which information the chunk retrieval should target.
It also supplies the necessary information for the technique to
identify the targeted information chunk in a build log. Each chunk
retrieval technqiue has a specific \textit{granularity}, i.e.,\ the
smallest piece of text it can return (e.g., a line, or a word). The
granularity might be adjustable by configuration. \emph{Running a
chunk retrieval technique} means to execute a fully configured
technique to consumes as input a build log in plain text format and to
produce an array as output. The array consists of substrings of the
build log text.

\Cref{tab:ctr} shows a comparison of the presented techniques.

\begin{table*}[]
\centering
\caption{Overview of the described chunk retrieval techniques.}
\begin{tabularx}{\textwidth}{@{}XlXlXX@{}} 
\toprule
Name                         & Acronym & Identification Technique                                   & Granularity & Configuration & Source            \\ 
\midrule
Program Synthesis by Example & PBE     & Regular expression program                                 & Character   & In/output examples      \\
Common Text Similarity       & CTS     & TF-IDF \& cosine similarity, expected number of lines & Line        & Output examples           \\
Keyword Search               & KWS     & Keywords, expected number of lines                    & Line        & Keywords, context length  \\
Random Line Retrieval        & RLR     & Random sample                                              & Line        & Retrieval length          \\
Diff Approach                & ---     & Line not present in successful log, information retrieval  & Line        & Logs from failing and successful builds      \\
\bottomrule
\end{tabularx}
\label{tab:ctr}
\end{table*}

\subsection{Program Synthesis by Example (PBE)}
\label{sec:expl-pbe}
% \emph{Programming by Example} is a technique which synthesizes
% programs according to in- and output examples provided by the user.
% It enables users to create programs without a priori programming
% knowledge~\cite{mayer2015user}. In the context of text extraction
% through regular expressions, Programming by Example relieves the
% developer from having to understand the whole document structure to
% solve a single extraction task~\cite{le2014flashextract:}. In this
% work, we refer to our interpretation of Programming by Example as
% \emph{PBE}\@. We investigate the suitability of PBE to retrieve
% information chunks from build logs. In the following, we explain the
% configuration and application of PBE to chunk retrieval from CI
% build logs.

% TODO later: we should make sure to at least mention PROSE
% _somewhere_, it's crucial for why PBE is how it is
\subsubsection{Configuration}
In- and output example pairs are the main driver of Programming by
Example, we refer to them in short as \emph{examples}. The
\emph{input} is the text of the build log file. The \emph{output} is
an array of substrings of the log file text, representing the
substrings that should be retrieved by the synthesized program when
given the corresponding input file. One or multiple examples, the
training set, \emph{configure} a specific chunk retrieval with PBE:
they define the substring of a build log that should be extracted. The
PROSE program synthesis then tries to construct a program consistent
with all training examples.
% A program is consistent with an example if it returns the defined
% output when executed on the defined
% input~\cite{mitchell1982generalization}.
PBE reports an error back to the user if it could not synthesize a
consistent program.
% The program synthesis builds on the FlashExtract DSL, which in turn
% uses the FlashMeta algorithm. Both are described in
% Section~\ref{sec:rw-prose}.


% TODO Moritz: you said: The following is very obvious and could apply to any
% technique. 
% kinda. It does give a descriptive error message on no match (and
% that is not possible in other techniques)
% otherwise yes, nothing interesting happens in the application of pbe
% though for consistency we might need this section? or merge config/application?
\subsubsection{Application}
A run of PBE takes a build log file as input and applies the
synthesized regular expression program. It then returns the substring
of the build log matched by the program or an error message if the
program found no match.


\subsection{Common Text Similarity (CTS)}
\label{sec:expl-ts}
% Text Similarity approaches are used to filter unstructured textual
% software
% artifacts~\cite{runeson2007detection,marcus2005recovery,antoniol2002recovering,mccarey2006recommending}.
% One common and simple technique is the Vector Space Model. We
% investigate when text similarity is a suitable technique to retrieve
% information chunks from build logs. In the following we will explain
% the concept of how we apply text similarity to information retrieval
% from CI build logs, which we refer to as \emph{CTS}\@.

\subsubsection{Configuration}
To configure chunk retrieval though text similarity we chose to use
the same concept of examples as for PBE
% TODO Moritz: your question was: More details on this. How do you
% 'apply' VSM? On what?
% all the following sentences in this subsubsection describe our way
% of determining text similarity which as far as I understand and
% Annibale said is the VSM. word tokens, tf-idf, similarity measure
% I added a "as follows", think that is enough for explanation? 
and apply the Vector Space Model~\cite{schutze2008introduction} as follows. The
lines of the output strings of the training examples define our search
query. The algorithm splits the search query into single lines and
identifies tokens, in our case words. Then we build a
document-term-frequency matrix over the lines from the search query
and prune very often or very rarely appearing words. Finally, the
algorithm applies TF-IDF to the matrix, a best practice for natural
language queries~\cite{lee1997document}.

\subsubsection{Application}
To retrieve the desired information from a build log, we parse the
whole text and process it in the same way as the output of the
training examples. The algorithm calculates the cosine
similarity~\cite{korenius2007principal} to compare each line of the
build log with each line of the search query. After summing up the
similarities of each build log line to all search query lines, we sort
the build log lines in decreasing similarity. The average number of
lines in the outputs of the training examples determines how many of
the most similar lines are returned as the output of the retrieval
run.

\subsection{Keyword Search (KWS)}
\label{sec:expl-skws}
When developers scavenge for a specific piece of information within a
large amount of unstructured information, a first ad-hoc approach they
use is to search for related keywords. Indeed, this was one of the
most common approaches we took when searching for the reason the build
failed within a log while creating the \emph{LogChunks} data
set~\cite{brandt2020logchunks}.
% As this is a technique readily available in many tools developers
% use to view build logs, we study when such a keyword search is
% suitable for retrieving information chunks from CI build logs. In
% the following we will explain how we use simple keyword search to
% retrieve information from CI build logs, which we refer to as
% \emph{KWS}\@.

\subsubsection{Configuration}
A set of keywords configures the chunk retrieval with KWS\@. To better
compare KWS with PBE and CTS, we also configure it through examples.
We associate each example with keywords which appear in the targeted
chunk or close to it. 
KWS then searches for those keywords which are tied to the greatest
number of examples in the training set.
If several keywords are associated with the same number of training
examples, and no other keywords are associated with more training
examples, KWS searches for all of these keywords.

\subsubsection{Application}
For a retrieval run, we take a whole build log file as input and
search for all exact occurrences of the keywords. As keywords are
often not directly describing the desired information, but rather
appear close to the desired information, KWS also retrieves the lines
around the found keyword. The number of surrounding lines retrieved is
the average of lines in the output of the training examples.


\subsection{Random Line Retrieval}
\label{sec:expl-rlr}
In our evaluation, we want to compare against a baseline of randomly
picking lines from the build log. RLR essentially mimicks the
situation of guessing purely blindly which lines might be interesting.
The only configuration option for RLR is the number of lines it should
return. For a fair comparison to the other techniques (whose number of
returned lines is dynamic), we configure RLR to return the average
number of lines in the chunks of the training examples. 
% TODO Moritz: you said: some more details here?
% I would not know of any details to add. It's really all we are doing

\subsection{Other Techniques}
%Literature mentions further build log analysis techniques. This
%section describes them with our notion of chunk retrieval techniques.

\noindent
\textbf{Log Diff}
Amar et al.\ use a technique based on line diffs and information
retrieval to identify relevant lines from a failed build
log~\cite{amar2019mining}, as we describe in more detail in
Section~\ref{sec:rw-bl-analysis}. The configuration for the technique
is the log from the last successful build and relevant past failures.
This technique retrieves the lines from a build log that are not
present in the successful build log and contain terms related to the
given past failures.

% TODO: we could add more techniques from the lit. survey here

\subsection{Tool Implementation}
For our comparison study we implemented PBE, CTS, KWS and RLR and a
unifying interface. The unified interface is implemented in Ruby and
calls the separate technique implementations over the command line.
The implementation of PBE in C\# is based on the Microsoft PROSE
library~\cite{prose2019webpage}. We implemented CTS, KWS and RLR using
R and the text2vec library~\cite{text2vec2019webpage}.

