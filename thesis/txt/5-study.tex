\providecommand{\myrootdir}{..}
\documentclass[\myrootdir/main.tex]{subfiles}

\begin{document}

\chapter{Empirical Comparison Study}
\label{sec:study}
To investigate when PBE, CTS and KWS are suited to retrieve chunks from CI build logs we evaluated our implementation of those techniques on the \emph{Failing Build Log Data Set}.
This chapter describes our study design and presents the results of the study.
The analysis of the study results first focusses on each of the three techniques and later we compare them against each other.


\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth, clip]{img/study-design.pdf}
	\caption{Study design of our technique comparison study}
	\label{fig:study}
\end{figure}

\section{Study Design}
For the comparison study we evaluate the three chunk retrieval techniques PBE, CTS and KWS, described in Sections \ref{sec:expl-pbe},\ref{sec:expl-ts} and \ref{sec:expl-skws}.
RLR, explained in Section \ref{sec:expl-rlr}, acts as a baseline for the comparison.
We run four techniques on the example sets from the \emph{Failing Build Log Data Set}.

The examples are sorted chronologically, i.e. a technique is configured with examples from the directly preceding build logs.
For each example set and each technique, we select one to five successive I/O examples as configuration, the \emph{configuring I/O examples}, and run the chunk retrieval on the next I/O example, the \emph{test I/O example}.

For each of our evaluation runs we measure the desired vs. actual output lines of the chunk retrieval.
The desired output is given by the output of the test I/O example, which acts as the oracle for our evaluation.
From this we obtain true/false positives/negatives and calculate precision, recall and accuracy, which we use to answer \textbf{RQ2.2}.
We register the number of examples used as configuration for each run to answer \textbf{RQ2.1} and their structural categories to answer \textbf{RQ2.3}.
\todo{mention / explain retrival size factor}
\todo{introduce special words / measurements like ``successful''?}

%taking The Failing Build Log Data Set - run 3(4 with random) techniques with increasing example count - measuring xyz - justify choices like running chronologically / testing on 1 example / no k-fold validation - how are keywords for the search selected?

\section{Results}
This section presents the evaluation results for PBE, CTS and KWS separately.
Afterwards it compares the three techniques with each other and RLR as baseline.

% \begin{figure}[htbp]
% 	\centering
% 	\includegraphics[width=\textwidth, clip]{img/big-study/categories-dataset.pdf}
% 	\caption{Distribution of Category Combinations in the Example Sets used for Configuration in our Study}
% 	\label{fig:categories-dataset}
% \end{figure}

% \begin{figure}[htbp]
% 	\centering
% 	\begin{minipage}{0.45\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth, clip]{img/big-study/categorycount-examplecount-dataset.pdf}
% 		\caption{Distribution of the Count of Categories within the Examples used for Configuration in our Study}
% 		\label{fig:categorycount-examplecount-dataset}
% 	\end{minipage}\hfill
% 	\begin{minipage}{0.45\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth, clip]{img/big-study/xxx.pdf}
% 		\caption{caption}
% 		\label{fig:xxxy}
% 	\end{minipage}
% \end{figure}

% \subsection{Category Distribution in Data Set}
% Categories distributed in over data set
% - why are we looking at that?: interesting how categories are distributed for this real world example of build Failure reasons in travis ci categorization
% - Figure \ref{fig:categories-dataset} shows the distribution of categories in the configuring example sets in rou study
% - Figure \ref{fig:categorycount-examplecount-dataset} shows the category count separated by the different example count.


\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth, clip]{img/big-study/failure-reason-PBE.pdf}
		\caption{Results of chunk retrieval with PBE}
		\label{fig:failure-reason-PBE}
	\end{minipage}\hfill
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth, clip]{img/big-study/failure-reason-categorycount-PBE.pdf}
		\caption{Results of chunk retrieval with PBE for numbers of categories present in configuring I/O examples}
		\label{fig:failure-reason-categorycount-PBE}
	\end{minipage}
\end{figure}
\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth, clip]{img/big-study/precision-extraction-result-PBE.pdf}
		\caption{Precision of chunk retrieval with PBE for increasing count of configuring I/O examples}
		\label{fig:precision-extraction-result-PBE}
	\end{minipage}\hfill
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth, clip]{img/big-study/xxx.pdf}
		\caption{xxx}
		\label{fig:xxx}
	\end{minipage}
\end{figure}
\subsection{Regular Expression Program Synthesis by Example (PBE)}
Figure~\ref{fig:failure-reason-PBE} shows the results of the PBE runs in our evaluation.
Out of the 400 runs, 5 per each one of the 80 example sets, PBE extracted all the desired lines in 138 cases.
Figure \ref{fig:failure-reason-PBE} shows that in 89 further cases a program was also successfully synthesized, though in 59 cases the synthesized program yielded no output at all.
In 30 cases the synthesized program did not extract all of the desired lines, though still had an average recall of 28\%.
In 173 cases the PROSE program synthesis could not synthesize a singular regular expression program that satisfies all of the configuring I/O examples.

Figure \ref{fig:failure-reason-categorycount-PBE} shows the results of PBE runs compared to the number of categories present in the configuring I/O examples.
It shows that the program synthesis is more likely to succeed when there are few categories present in the configuring I/O examples.
When two categories present, PROSE could not synthesize a program consistent with all configuring I/O examples in most cases.
For three or more categories PROSE could never synthesize a consistent program.

Figure \ref{fig:precision-extraction-result-PBE} compares the precision of the PBE chunk retrieval runs with the count of configuring I/O examples.
For one example there is a high number of empty extraction outputs, meaning that the regular expression synthesized from this one I/O example could not be applied to the test I/O example.
For two or more I/O examples the chunk retrieval is either successful or no consistent program could be synthesized.


\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth, clip]{img/big-study/recall-precision-examplecount-TS.pdf}
		\caption{Precision and recall of chunk retrieval with CTS for increasing count of configuring I/O examples}
		\label{fig:recall-precision-examplecount-TS}
	\end{minipage}\hfill
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth, clip]{img/big-study/recall-precision-categorycount-TS.pdf}
		\caption{Precision and recall of chunk retrieval with CTS for increasing number of categories present in the configuring I/O examples}
		\label{fig:recall-precision-categorycount-TS}
	\end{minipage}
\end{figure}
\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth, clip]{img/big-study/contextsizefactor-precision-recall-TS.pdf}
		\caption{Precision and recall of chunk retrieval with CTS compared to retrieval size factors}
		\label{fig:contextsizefactor-precision-recall-TS}
	\end{minipage}\hfill
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth, clip]{img/big-study/xxx.pdf}
		\caption{xxx}
		\label{fig:xxx}
	\end{minipage}
\end{figure}

\subsection{Common Text Similarity (CTS)}
Figure \ref{fig:recall-precision-examplecount-TS} presents precision and recall of chunk retrieval using CTS in our study for different numbers of configuring I/O examples.
The count of configuring I/O examples has no visible influence on recall and precision of the chunk retrieval with CTS.

Figure \ref{fig:recall-precision-categorycount-TS} shows the same measurements for different category counts in the configuring I/O examples.
With increasing category count, precision and recall decrease.
Especially for more than three categories present we have no chunk retrieval runs where all desired lines were extracted.

Figure \ref{fig:contextsizefactor-precision-recall-TS} shows the impact of the retrieval size factor on precision and accuracy of chunk retrieval runs with CTS.
The precision ranges from 52\% when retrieving half expected number of lines to 25\% when 2.5 times the expected number of lines.
The recall ranges from 30\% to 55\%.


\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth, clip]{img/big-study/recall-precision-examplecount-SKWS.pdf}
		\caption{Precision and recall of chunk retrieval with SKWS for increasing count of configuring I/O examples}
		\label{fig:recall-precision-examplecount-SKWS}
	\end{minipage}\hfill
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth, clip]{img/big-study/recall-precision-categorycount-SKWS.pdf}
		\caption{Precision and recall of chunk retrieval with SKWS for increasing number of categories present in the configuring I/O examples}
		\label{fig:recall-precision-categorycount-SKWS}
	\end{minipage}
\end{figure}
\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth, clip]{img/big-study/contextsizefactor-precision-recall-SKWS.pdf}
		\caption{Precision and recall of chunk retrieval with SKWS compared to retrieval size factors}
		\label{fig:contextsizefactor-precision-recall-SKWS}
	\end{minipage}\hfill
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth, clip]{img/big-study/xxx.pdf}
		\caption{xxx}
		\label{fig:xxx}
	\end{minipage}
\end{figure}


\subsection{Keyword Search (KWS)}
\todo{plot accuracy? especiall against retrieval size factor, could be that we are soon extracting all/most avaliable lines}
Figure \ref{fig:recall-precision-examplecount-SKWS} presents precision and recall of chunk retrieval using SKWS in our study for different numbers of configuring I/O examples.
The recall jumps from 53\% to over 70\% for more than two configuring I/O examples, while the precision stays constant at around 16\%.

Figure \ref{fig:recall-precision-categorycount-SKWS} shows the same measurements for different category counts in the configuring I/O examples.
There is no clear trend visible in precision and recall for an increasing amount of categories in the configuring I/O examples

Figure \ref{fig:contextsizefactor-precision-recall-SKWS} shows the impact of the retrieval size factor on precision and accuracy of chunk retrieval runs with SKWS.
The precision ranges from 19\% when retrieving half expected number of lines to 10\% when 2.5 times the expected number of lines.
The recall ranges from 58\% to 75\%.


\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth, clip]{img/big-study/precision-all.pdf}
		\caption{Precision \todo{and recall} of chunk retrieval with PBE, CTS and KWS compared with RLR}
		\label{fig:precision-all}
	\end{minipage}\hfill
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth, clip]{img/big-study/precision-category-singularity-all.pdf}
		\caption{Precision \todo{and recall} of chunk retrieval with PBE, CTS and KWS compared with RLR for single and multiple categories present in the configuring I/O examples}
		\label{fig:precision-category-singularity-all}
	\end{minipage}
\end{figure}
\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth, clip]{img/big-study/success-all.pdf}
		\caption{Successful chunk retrievals for PBE, CTS, KWS and RLR}
		\label{fig:success-all}
	\end{minipage}\hfill
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth, clip]{img/big-study/xxx.pdf}
		\caption{\todo{pointplot precision and recall for all techniques compared, also aganist example/category count?}}
		\label{fig:xxx}
	\end{minipage}
\end{figure}

\subsection{Comparing All Techniques}
Figure \ref{fig:precision-all}: PBE highest precision if extraction successful, also highest recall of all?. CTS still good also when not all information extracted

Figure \ref{fig:precision-category-singularity-all}: pbe very good for a single category, very bad for multilpe, performance of others quite similar for single vs. multiple categories, CTS slightly impacted, random benefits (<- do we need to explain that in text?)

Figure \ref{fig:success-all} pbe and kws best at extracting all desired lines, precision fig: pbe with higher precision

\end{document}
