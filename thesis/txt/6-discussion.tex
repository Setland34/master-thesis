
\providecommand{\myrootdir}{..}
\documentclass[\myrootdir/main.tex]{subfiles}

\begin{document}
\chapter{Discussion}

This chapter answers our second research question and its subquestions:
\begin{simplebox}{Research Questions}
\begin{itemize}
  \item[\textbf{RQ2:}] When are PBE, CTS, and KWS suited to retrieve information from CI build logs?
  \item[\textbf{RQ2.1:}] How many examples do PBE, CTS, and KWS need to perform best?
  \item[\textbf{RQ2.2:}] How accurate are the retrievals of PBE, CTS, and KWS?
  \item[\textbf{RQ2.3:}] How structurally similar do the examples for PBE, CTS and KWS need to be for the techniques to be applicable?
\end{itemize}
\end{simplebox}

The first section discusses for PBE, CTS and KWS separately in which cases they perform best.
It details for which types of input build logs, available configuration I/O examples and use case of the retrieved output each technique is suited.
The next section compares the three techniques and clarifies which one is recommended by our study results for different use cases.

\section{Interpretation of Study Results}
This section discusses the study results for each of the analyzed chunk retrieval techniques separately.
It gives recommendations on which kind of configuration is best for each technique and for what kind of usage the output is suitable.

\subsection{Regular Expression Program Synthesis by Example (PBE)}
\paragraph{Configuration and Input}
Our study results show, that chunk retrieval with PBE gives best results when the configuring I/O examples are from one structural category.
This means it is suited to retrieve BLIs, whose textual representation always has the same surrounding structure.
To extract for example the reason a build failed, the log passage describing the failure would always have to be started and ended the same way.

When all configuring I/O Examples are of the same structure, one to three examples are enough input for PROSE to synthesize a correct regular expression program. In our study, additional examples did not improve the quality of teh chunk retrieval.

\paragraph{Retrieval Output Usage}
When the program synthesis succeeds and applying the regular expression program yields an output, PBE shows a high precision and high recall for chunk retrieval.
A failure in the program synthesis or no output from the synthesized program is clearly identified by the tool.
Therefore, if there is an output, the user can have high confidence it is the correct output.
This makes output from PBE chunk retrieval well suited for consumption of other software components,

\subsection{Common Text Similarity (CTS)}
\paragraph{Configuration and Input}
Chunk retrieval using CTS also yields good results if more than one structural category is present in the configuring I/O examples.
The number of configuring I/O examples had no measurable influence on the extraction quality in our study.
Information retrieval techniques like text similarity commonly learn on a higher number of examples than we used for our study.
Future work is needed to investigate how many examples yield improvements in the chunk retrieval over a single configuring I/O example.

Extracting the average number of lines present in the configuring I/O examples seems to give the best balance of precision and recall for CTS.

\paragraph{Retrieval Output Usage}
CTS has good precision and recall on average, though the quality of a chunk retrieval run is very hard to predict from the given result.
Therefore, retrieval output from CTS is suited to be read by a human.

\subsection{Keyword Search (KWS)}
\paragraph{Configuration and Input}
The number of structural categories in the configuring I/O examples have little impact on the extraction quality of KWS.
This makes KWS a good technique if there is little prior knowledge on how the desired BLI is represented in the build log to be analyzed.
For the example of extracting the build failure reason, KWS is best suited if a build can fail in various steps logged by different tools and no pre-categorization of where the build failed is available.

To achieve good recall of the retrieval, at least two examples should be given as configuration.

Retrieving the average number of lines present in the outputs of the configuring I/O examples around every found keyword yields reasonable recall.
Selecting 1.5 times as many lines around every found keyword does improve the recall in our study but also increases the proportion of lines retrieved overall and therefore decreases precision.

\paragraph{Retrieval Output Usage}
Even though KWS has the highest recall of all three techniques, its precision is also the lowest.
The output of a chunk retrieval with KWS is well suited to be read by humans, matching our 
very low precision, only useful to filter lines in build log for further processing, e.g. reading by a developer

\section{Choosing a Suitable Chunk Retrieval Technique}
This section unifies our study results of the chunk retrieval techniques PBE, CTS and KWS.
It recommends which technique to use depending on the available configuration and 
TABLE!
\paragraph{Configuration and Input}
number of categories:
one? -> pbe
two? -> CTS
more? -> KWS
available examples:
little? -> try pbe (This depends on the categories! here we might want a decision tree to represent the importance of these characteristics against each other? <- this would also anwser RQ1: which ones of the criteria we compare are the most important)
multiple -> kws
very many? -> CTS

\paragraph{Retrieval Output Usage}
precision needed:

\begin{table}[htbp]
\begin{tabular}{ |c||c|c|c| }
  Technique & \multicolumn{3}{|c|}{Number of Categories in configuring examples} \\
  \hline
  & one & two & more \\
  PBE & yes & no & no \\ 
  CTS & yes & yes & no \\ 
  KWS & yes & yes & yes \\ 
  \hline
\end{tabular}
\caption{The small $1 \cdot 1$}
\label{tab:101}
\end{table}

\section{Threats to Validity}
chronologically selected examples -> very specific view, devs/users might pick more representative examples

very few evaluation runs with a high number of categories in configuring I/O examples -> because we chose to create such a ``realistic'' data set, from data set we see that many structural categories uncommon

\end{document}
