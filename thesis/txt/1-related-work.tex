
\providecommand{\myrootdir}{..}
\documentclass[\myrootdir/main.tex]{subfiles}

\begin{document}

\chapter{Background}
\plan{all of RW: 4 - 5 pages left to do + some research!}
\plan{write, 1/3 page, blocked by: other sections}
\label{sec:rw}
This section presents various research works adjacent or foundational to our work.
Starting with studies about Continuous Integration used in software projects, we move on to past works about build log analysis and their augmentation.
We differentiate our work from the more common system log analysis and show how we classify build logs as semi-structured data.
We cover different information retrieval and extraction techniques \todo{half sentence missing: specify which techniques? 'used in software reasearch' 'used to assist software engineers'}.
Lastly this section mentions different Programming by Example resources surrounding Microsoft's work on the PROSE library.
\todo{do we also mention more things?}
% moritz: nice formulation :)

\section{Continuous Integration}
\plan{extend, 1 page}
\mention{look into proksch papers}
Various researchers have analyzed industrial and open source logs for failure reasons and their impact on development. Seo et al.~\cite{seo2014programmers} found that few error types such as dependency mismatches are the most prominent cause of build failures at Google. In addition most failures are resolved within two builds. Vassallo et al.~\cite{vassallo2017a-tale} compared open source projects in Java to industrial ones. They determined that testing failures outweigh compilation errors. Open source builds fail most often because of unit tests, whereas release preparations is the primary cause in industrial projects. Beller et al.~\cite{beller2017oops} showed that testing is central to continuous integration when evaluating Travis CI logs for Java and ruby builds. They observed very different kinds, failure rates and numbers of test between programming languages and explained that the low failure rates hint at code being tested before it is sent to the CI server.

All these researchers described building parsers in order to evaluate the studied build logs. Our work could support their research by easing the parser development and enable them to cover more languages and build tools easier.

\section{Build Log Analysis and Augmentation - Related Work}
\label{sec:rw-bl-analysis}
\plan{extend, 1 page}
Vassallo et al.~\cite{vassallo2018un-break} tried to shorten the time it takes developers to understand build logs. They summarized relevant information in Maven build logs and augmented them with links to related stack overflow posts. They observed that highlighting the locality and context of an issue is helpful to programmers. We strive to enable a similar summarization by text retrieval while also covering a wider array of programming languages.
\todo{mention which technique}
\todo{reference their new paper}

Amar et al.~\cite{amar2019mining} reduced the lines \todo{portions?} of a log to be inspected by the engineer through removing lines that appear both in passing and failing build logs. They further employed information retrieval techniques to identify the lines most likely hinting at the cause of the error. In contrast to that, our tool ALBE extracts specific parts of the build logs. As this is mostly dependent on the implicit reoccurring structure within the logs we operate on the full log output.

\section{System Log Analysis}
\plan{research + write, 1/2 - 1 page}
\mention{logpai and drain, [7] K. Fisher and D. Walker. The pads project: an overview. In ICDT,
2011.
[8] K. Fisher, D. Walker, K. Q. Zhu, and P. White. From dirt to shovels:
fully automatic tool generation from ad hoc data. In POPL, 2008.[23] Q. Xi and D. Walker. A context-free markup language for semi- structured text. In PLDI, pages 221–232, 2010.
 The PADS project [7] has en- abled simplification of ad hoc data processing tasks for programmers
by contributing along several dimensions: development of domain specific languages for describing text structure or data format, learn- ing algorithms for automatically inferring such formats [8], and a markup language to allow users to add simple annotations to enable more effective learning of text structure [23] While PADS supports parsing of entire files, FlashExtract allows users to extract only parts of the file thereby avoiding unnecessary complications. PADS’s learner only supports a fixed line-by-line chunking strategy to split the records; in contrast, FlashExtract can learn chunking (aka, struc- ture boundaries) from examples, making it suitable for extracting data fields and records that have arbitrary length (and might cross multiple lines). Finally, PADS primarily targets ad hoc text files. Although one can view webpages and spreadsheet as text files, it is unclear if the PADS learning algorithm can be adapted to work effectively for webpages and spreadsheets.
}

\mention{ask Jean?}

\section{Semi-Structured Data}
\label{sec:rw-semi-structured-data}
Serge Abiteboul introduced a theory of semi-structured data in his report from 1997 about integrating data from several sources~\cite{abiteboul1997querying}.
He proposes essential characteristics of semi-structured data, and we show how five of these map to the context of build logs.

The structure of build logs
\begin{itemize}
  \item \textbf{is implicit}.
  We might not have access to explicit structuring elements or an explicit structure description.
  Computation is required to infer the present structure.
  \item \textbf{is irregular}.
  Changes in the build process or execution environment might change the structure of a build log.
  We observed this in the logs we collected for the \emph{Failing Build Log Data Set}, where for the same repository and build configuration some logs had double new line characters without any noticeable explanation.
  You can see an example of this in Figure~\ref{fig:log-4} and Figure~\ref{fig:log-5}.
  \item \textbf{is partial}. Some parts are highly structured by e.g.\ structure indicating special characters and others are unstructured, e.g.\ natural language text in error messages.
  \item can be described with an \textbf{analytical data guide} rather than a beforehand defined schema.
  There is no fixed specification on how build tools structure their output.
  We afterwards extract structuring patterns in the produced output.
  \item \textbf{has a rapidly evolving schema}.
  Modifications in the build configuration might change the tools involved in the build and therefore the composition of the build log.
  Previous work indicates that some projects change their CI configuration often~\cite{hilton2016usage} and software tools adopt their log messages over time~\cite{yuan2012characterizing}.
\end{itemize}

Abiteboul proposes an approach to overlay the semi-structured data with a structured layer.
The additional layer can answer queries and give access to the semi-structured data relevant to the query.
In contrast to that, this thesis takes a look at techniques to gater a specific, pre-specified information without the need to parse, understand or estimate the whole structure of a log.

\section{Information Extraction and Retrieval Techniques}
\plan{research + write, 1/2 - 1 page}
\todo{explain difference here, reference pads(maybe that is already in log anlysis!) and stuff}
\review{Getting the general tot p cic or conceptual information of a text is a common task in information retrieval from semi-structured text sources. Usually this is done by preprocessing the documents, transforming them to a term-by-document matrix. On the matrix we apply a similarity comparison like for example vector space models to calculate the similarity of the different documents to each other~\cite{panichella2016parameterizing}. For our use case, this could be applied by slicing the build logs into lines or small sections. Then similarity measures are used to compare the overall topics in these subparts to the topic of previously labelled logparts.
Instead of receiving a paragraph which is similar to a given query, our works focus more on obtaining specific pieces of texts through regular expressions.}



\mention{IR papers, maybe something about keyword search?}

\section{Program Synthesis by Example}
\label{sec:rw-prose}
Programming by Example enables end users to automate repetitive tasks.
The user provides examples for the input and the corresponding output and a synthesis algorithm tries to create the program intended by the user.
This section introduces the theoretical foundations of the program synthesis algorithm implemented in the \emph{Microsoft Program Synthesis using Examples} (PROSE) framework~\cite{prose2019webpage}.
It presents the FlashExtract DSL, which defines text extraction tasks within PROSE and is the basis for the implementation of our PBE chunk retrieval technique.
Further, this section covers further applications and extensions of the PROSE program synthesis, as well as previous work about 


\paragraph{FlashMeta: Inductive Program Synthesis}
The FlashMeta framework presented by Polozov and Gulwani~\cite{polozov2015flashmeta:} is the backbone of the program synthesis in the Microsoft PROSE framework.
FlashMeta separates the inductive synthesis algorithm from the domain specific capabilities of the desired program, by encoding the possible program space in a domain specific language (DSL).
The user specifies their intent by providing in / output examples (I/O examples).
FlashMeta uses \emph{witness functions}, provided by the DSL, to divide the synthesis into smaller subtasks.
For each of these subtasks it enumerates all possible programs that solve the subtask consistent with the set of I/O examples.
A program is consistent with a set of I/O examples if it produces the corresponding output for each input example~\cite{mitchell1982generalization}.
The subprograms are stored in a \emph{version space algebra} (VSA)~\cite{mitchell1982generalization}, which efficiently saves candidate programs for tasks by sharing common subexpressions.
The enumerated programs are ranked according to which ones the user most likely intended.
The ranking characteristics are also provided by the DSL.
From the ranked VSA, FlashMeta can then return a ranked list of complete programs consistent with the users example.

The different applications of PROSE presented in the following paragraphs are all implemented as DSLs for the FlashMeta synthesis algorithm.


\paragraph{FlashExtract: Data Extraction by Example}
Le et al.~\cite{le2014flashextract:} developed FlashExtract as DSL for the Microsoft PROSE framework.
Its instantiation for text specifies text extraction programs from semi-structured text based on regular expressions.
The authors also provide instantiations to extract data from webpages and spreadsheets.
Users can extract multiple fields and structure them with hierarchy and sequence.
FlashExtract synthesizes programs to extract each of the fields leveraging the information about hierarchical containment and sequentiality.
It eliminates the need for the user to understand the entire structure of the processed document and the effort of developing a suitable extraction program themselves.

The text instantiation of FlashExtract models the extraction of a single substring as a pair of two cut positions.
A position is either specified by an absolute character index or a pair of two regular expression (regex).
The first regex matches the substring directly before the characterized position, the second regex directly after.
A regex in FlashExtract is a concatenation of tokens, e.g.\ standard character classes or string literals frequently occurring in the input examples.

We apply the text instantiation of FlashExtract to the domain of build logs, using programming by example to take away the need to tediously develop and maintain regular expressions for information retrieval.


\paragraph{Other Applications for Program Synthesis by Example}
refazer, program transformations~\cite{rolim2017learning}
spred sheets ~\cite{gulwani2011automating}
table transformations~\cite{harris2011spreadsheet}
SOME MISSING

\paragraph{Improving the Program Synthesis}
predictive data extraction~\cite{raza2017automated}
instead of only attributing program simplicity by program syntax e.g. shortness also take exectution traces (e.g. which substrings extraction sare performed) and output similarity to each other~\cite{ellis2017learning}

\paragraph{Perspective: Usability vs. Technical Aspects} maybe: usability perspecitve / usability of pbe \todo{REWORK}
Generally, we are interested in whether Programming by Example could replace manually writing regular expressions.
Writing and especially maintaining regular expressions is known to be a difficult, tedious and error-prone task~\cite{michael2019regexes}.
There are two ways to approach this question: from the usability side or the technical side.
When studying the applicability of PBE from the \emph{usability} perspective, typical questions are:
\begin{itemize}
	\item Are users more efficient when using PBE compared to manual regex construction?
	\item Are users more comfortable using PBE over manual regex construction?
	\item Are users confident in the resulting regular expression program from PBE?
	\item Are the resulting extractions more or less accurate when using PBE over manual regex construction?
\end{itemize}
Some of these are evaluated in existing works about PBE and user interaction~\cite{mayer2015user,lau2009why-programming-by-demonstration,miller2001outlier}.
% Adequately answering those questions is however highly dependent on the user interface and the corresponding interaction model used during a study.
% Developers who are accustomed to crunching on regular expressions in their favorite IDE or Editor might have an adversely negative view on synthesizing programs from examples in an unknown interface.
% Mayer et al.~\cite{mayer2015user} presented two system-user interaction models to improve the confidence of users into the programs synthesized from their examples.
% We feel such questions of usability can be answered separately from the application domain of build logs.

For our work, we chose to look at the applicability of PBE from the \emph{technical} perspective.
We investigate whether current program synthesis techniques can learn programs for information retrieval tasks in the domain of CI build logs.
Furthermore, we gather insights into how PBE has to be applied to yield useable results and perform best.
Our implementation of PBE is mainly based on the Flash Extract DSL~\cite{le2014flashextract:} of the PROSE library by Microsoft~\cite{prose2019webpage}.
Due to that our technique of text extraction through regular expression programs synthesized is highly influenced by the capabilities of Flash Extract.

\end{document}
