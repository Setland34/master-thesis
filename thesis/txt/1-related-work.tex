
\providecommand{\myrootdir}{..}
\documentclass[\myrootdir/main.tex]{subfiles}

\begin{document}

\chapter{Background}
\plan{all of RW: 4 - 5 pages left to do + some research!}
\plan{write, 1/3 page, blocked by: other sections}
\label{sec:rw}
This section presents various research works adjacent or foundational to our work.
Starting with studies about Continuous Integration used in software projects, we move on to past works about build log analysis and their augmentation.
We differentiate our work from the more common system log analysis and show how we classify build logs as semi-structured data.
We cover different information retrieval and extraction techniques \todo{half sentence missing: specify which techniques? 'used in software reasearch' 'used to assist software engineers'}.
Lastly this section mentions different Programming by Example resources surrounding Microsoft's work on the PROSE library.
\todo{do we also mention more things?}
% moritz: nice formulation :)

\section{Continuous Integration}
\plan{extend, 1 page}
\mention{look into proksch papers}
Various researchers have analyzed industrial and open source logs for failure reasons and their impact on development. Seo et al.~\cite{seo2014programmers} found that few error types such as dependency mismatches are the most prominent cause of build failures at Google. In addition most failures are resolved within two builds. Vassallo et al.~\cite{vassallo2017a-tale} compared open source projects in Java to industrial ones. They determined that testing failures outweigh compilation errors. Open source builds fail most often because of unit tests, whereas release preparations is the primary cause in industrial projects. Beller et al.~\cite{beller2017oops} showed that testing is central to continuous integration when evaluating Travis CI logs for Java and ruby builds. They observed very different kinds, failure rates and numbers of test between programming languages and explained that the low failure rates hint at code being tested before it is sent to the CI server.
~\cite{vassallo2019every}
All these researchers described building parsers in order to evaluate the studied build logs. Our work could support their research by easing the parser development and enable them to cover more languages and build tools easier.

\section{Build Log Analysis and Augmentation - Related Work}
\label{sec:rw-bl-analysis}
\plan{extend, 1 page}
Vassallo et al.~\cite{vassallo2018un-break} tried to shorten the time it takes developers to understand build logs. They summarized relevant information in Maven build logs and augmented them with links to related stack overflow posts. They observed that highlighting the locality and context of an issue is helpful to programmers. We strive to enable a similar summarization by text retrieval while also covering a wider array of programming languages.
\todo{mention which technique}
\todo{reference their new paper}

Amar et al.~\cite{amar2019mining} reduced the lines \todo{portions?} of a log to be inspected by the engineer through removing lines that appear both in passing and failing build logs. They further employed information retrieval techniques to identify the lines most likely hinting at the cause of the error. In contrast to that, our tool ALBE extracts specific parts of the build logs. As this is mostly dependent on the implicit reoccurring structure within the logs we operate on the full log output.

\section{System Log Analysis}
\plan{research + write, 1/2 - 1 page}
\mention{logpai and drain, [7] K. Fisher and D. Walker. The pads project: an overview. In ICDT,
2011.
[8] K. Fisher, D. Walker, K. Q. Zhu, and P. White. From dirt to shovels:
fully automatic tool generation from ad hoc data. In POPL, 2008.[23] Q. Xi and D. Walker. A context-free markup language for semi- structured text. In PLDI, pages 221–232, 2010.
 The PADS project [7] has en- abled simplification of ad hoc data processing tasks for programmers
by contributing along several dimensions: development of domain specific languages for describing text structure or data format, learn- ing algorithms for automatically inferring such formats [8], and a markup language to allow users to add simple annotations to enable more effective learning of text structure [23] While PADS supports parsing of entire files, FlashExtract allows users to extract only parts of the file thereby avoiding unnecessary complications. PADS’s learner only supports a fixed line-by-line chunking strategy to split the records; in contrast, FlashExtract can learn chunking (aka, struc- ture boundaries) from examples, making it suitable for extracting data fields and records that have arbitrary length (and might cross multiple lines). Finally, PADS primarily targets ad hoc text files. Although one can view webpages and spreadsheet as text files, it is unclear if the PADS learning algorithm can be adapted to work effectively for webpages and spreadsheets.
}

\mention{ask Jean?}

\section{Semi-Structured Data}
\label{sec:rw-semi-structured-data}
Serge Abiteboul introduced a theory of semi-structured data in his report from 1997 about integrating data from several sources~\cite{abiteboul1997querying}.
He proposes essential characteristics of semi-structured data, and we show how five of these map to the context of build logs.

The structure of build logs
\begin{itemize}
  \item \textbf{is implicit}.
  We might not have access to explicit structuring elements or an explicit structure description.
  Computation is required to infer the present structure.
  \item \textbf{is irregular}.
  Changes in the build process or execution environment might change the structure of a build log.
  We observed this in the logs we collected for the \emph{Failing Build Log Data Set}, where for the same repository and build configuration some logs had double new line characters without any noticeable explanation.
  You can see an example of this in Figure~\ref{fig:log-4} and Figure~\ref{fig:log-5}.
  \item \textbf{is partial}. Some parts are highly structured by e.g.\ structure indicating special characters and others are unstructured, e.g.\ natural language text in error messages.
  \item can be described with an \textbf{analytical data guide} rather than a beforehand defined schema.
  There is no fixed specification on how build tools structure their output.
  We afterwards extract structuring patterns in the produced output.
  \item \textbf{has a rapidly evolving schema}.
  Modifications in the build configuration might change the tools involved in the build and therefore the composition of the build log.
  Previous work indicates that some projects change their CI configuration often~\cite{hilton2016usage} and software tools adopt their log messages over time~\cite{yuan2012characterizing}.
\end{itemize}

Abiteboul proposes an approach to overlay the semi-structured data with a structured layer.
The additional layer can answer queries and give access to the semi-structured data relevant to the query.
In contrast to that, this thesis takes a look at techniques to gater a specific, pre-specified information without the need to parse, understand or estimate the whole structure of a log.

\section{Information Extraction and Retrieval Techniques}

The process of automatically selecting documents related to a given \emph{search query} is called \emph{Information Retrieval}~\cite{schutze2008introduction}.
%Text Similarity approaches are more and more used to filter unstructured textual software artifacts~\cite{runeson2007detection,marcus2005recovery,antoniol2002recovering,mccarey2006recommending}.

\emph{Information Extraction} techniques aim to automatically structure unstructured information to support subsequent processing.
The IEPAD tool created by Chang et al.~\cite{chang2003automatic} automatically identifies data extraction patterns in semi-structured web pages, without requiring user-labeled training examples.
PADS: stuff one section up^

We investigate chunk retrieval techniques which do not aim at inferring the whole structure of a build log, but instead focus on extracting one information characterized by a user.


\plan{research + write, 1/2 - 1 page}
\todo{explain difference here, reference pads(maybe that is already in log anlysis!) and stuff}
\review{Getting the general tot p cic or conceptual information of a text is a common task in information retrieval from semi-structured text sources. Usually this is done by preprocessing the documents, transforming them to a term-by-document matrix. On the matrix we apply a similarity comparison like for example vector space models to calculate the similarity of the different documents to each other~\cite{panichella2016parameterizing}. For our use case, this could be applied by slicing the build logs into lines or small sections. Then similarity measures are used to compare the overall topics in these subparts to the topic of previously labelled logparts.
Instead of receiving a paragraph which is similar to a given query, our works focus more on obtaining specific pieces of texts through regular expressions.}



\mention{IR papers, maybe something about keyword search?}

\section{Program Synthesis by Example}
\label{sec:rw-prose}
Programming by Example enables end users to automate repetitive tasks.
The user provides examples for the input and the corresponding output and a synthesis algorithm tries to create the program intended by the user.
This section introduces the theoretical foundations of the program synthesis algorithm implemented in the \emph{Microsoft Program Synthesis using Examples} (PROSE) framework~\cite{prose2019webpage}.
It presents the FlashExtract DSL, which defines text extraction tasks within PROSE and is the basis for the implementation of our PBE chunk retrieval technique.
Further, this section covers further applications and extensions of the PROSE program synthesis.

\paragraph{FlashMeta: Inductive Program Synthesis}
The FlashMeta framework presented by Polozov and Gulwani~\cite{polozov2015flashmeta:} is the backbone of the program synthesis in the Microsoft PROSE framework.
FlashMeta separates the inductive synthesis algorithm from the domain specific capabilities of the desired program, by encoding the possible program space in a domain specific language (DSL).
The user specifies their intent by providing in / output examples (I/O examples).
FlashMeta uses \emph{witness functions}, provided by the DSL, to divide the synthesis into smaller subtasks.
For each of these subtasks it enumerates all possible programs that solve the subtask consistent with the set of I/O examples.
A program is consistent with a set of I/O examples if it produces the corresponding output for each input example~\cite{mitchell1982generalization}.
The subprograms are stored in a \emph{version space algebra} (VSA)~\cite{mitchell1982generalization}, which efficiently saves candidate programs for tasks by sharing common subexpressions.
The enumerated programs are ranked according to which ones the user most likely intended.
The ranking characteristics are also provided by the DSL.
From the ranked VSA, FlashMeta can then return a ranked list of complete programs consistent with the users example.

In addition to I/O examples of the intended program, the user can also provide examples with only input or negative input examples not to be processed by the synthesized program.

The different applications of PROSE presented in the following paragraphs are all implemented as DSLs for the FlashMeta synthesis algorithm.

\paragraph{FlashExtract: Data Extraction by Example}
Le et al.~\cite{le2014flashextract:} developed FlashExtract as DSL for the Microsoft PROSE framework.
Its instantiation for text specifies text extraction programs from semi-structured text based on regular expressions.
The authors also provide instantiations to extract data from webpages and spreadsheets.
Users can extract multiple fields and structure them with hierarchy and sequence.
FlashExtract synthesizes programs to extract each of the fields leveraging the information about hierarchical containment and sequentiality.
It eliminates the need for the user to understand the entire structure of the processed document and the effort of developing a suitable extraction program themselves.

The text instantiation of FlashExtract models the extraction of a single substring as a pair of two cut positions.
A position is either specified by an absolute character index or a pair of two regular expression (regex).
The first regex matches the substring directly before the characterized position, the second regex directly after.
A regex in FlashExtract is a concatenation of tokens, e.g.\ standard character classes or string literals frequently occurring in the input examples.

Microsoft's system log analysis tool Azure Monitor lets users define custom log fields using the FlashExtract~\cite{azure2019custom}.
The ConvertFrom-String function in PowerShell allows a user to specify a example template for extracting hierarchical data from a text document~\cite{powershell2019convert}.

We apply the text instantiation of FlashExtract to the domain of build logs with our chunk retrieval technique PBE\@.

\paragraph{Other Applications of Program Synthesis by Example}
Rolim et al.~\cite{rolim2017learning} use code edits as examples to learn automatic program transformations.
Their DSL for PROSE abstracts over variables and subexpressions by describing rewrite rules applied to the abstract syntax tree.
The synthesized transformations proposes fixes for student assignments based on corrections from other students and to automates repetitive refactoring tasks.

In 2011 Gulwani and Harris applied a less generic predecessor of the FlashMeta framework to string manipulation within spreadsheets~\cite{gulwani2011automating} and spreadsheet transformations~\cite{harris2011spreadsheet}.

Raza and Gulwani~\cite{raza2017automated} present an automated algorithm that tries to predict data extraction programs from only input examples.
Their tool can split system log statements into table columns or extract data from lists on webpages into spreadsheets.

In a program synthesis shorter and simpler program often receive a better ranking.
Ellis and Gulwani~\cite{ellis2017learning} improved the accuracy of the PROSE program learner even further by also taking into account the execution traces of the program candidates.
For example, a program that extracts overlapping substrings is ranked lower than a program without overlapping extractions.
They also weigh the produced output of applying a program to input only examples and prefer programs which produces output structurally similar to the provided output examples.

\paragraph{User Interaction with Programming by Example}
I/O examples are an ambiguous specification of a program and user confidence in the correctness of the synthesized program is important for a wide adoption of Programming by Example based systems~\cite{lau2009why-programming-by-demonstration}.
Miller and Myers~\cite{miller2001outlier} uncover outliers in input data provided to a Programming by Example text editing task.
For the PROSE framework, Mayer et al.~\cite{mayer2015user} compare two approaches for disambiguation by the user.
With their first approach, the user can browse lower ranked program candidates in a tree like natural language transformation of the constructed VSA.
The second approach actively asks the user to resolve ambiguous output possibilities for input only examples.

\end{document}
